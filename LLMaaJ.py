import streamlit as st
from langfuse import Langfuse, observe, propagate_attributes
from dotenv import load_dotenv
from google import genai
from google.genai import types
import os
import tempfile
import json
from pathlib import Path
from datetime import datetime

# Load environment variables
load_dotenv(".env")

# Page configuration
st.set_page_config(
    page_title="Document Analyzer with Gemini",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize Langfuse
@st.cache_resource
def init_langfuse():
    return Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_BASE_URL"),
        timeout=300,
        sample_rate=1.0 ,
    )

langfuse = init_langfuse()

# Initialize Gemini client
@st.cache_resource
def init_gemini_client():
    return genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))

# Prompt templates
PROMPTS = {
    "Simple Markdown": """
## Task
Analyze the document image and provide:
1. **Transcribe** all text exactly as written
2. **Describe** any diagrams, tables, or visual elements

## Output
Respond in **Markdown format**:
- Use headings, lists, and tables to match the document structure
- For diagrams, describe the flow (e.g., A ‚Üí B ‚Üí C)
- Mark unclear text as `[unclear]`
""",
    "Detailed Markdown": """
# Document Analysis

## Task
Analyze the document and provide:
1. A markdown description of the page content (diagrams, tables, images, etc.)
2. Complete transcription of all text

## Output Format

### 1. Page Description
Provide a brief markdown summary describing:
- Document type and layout
- Visual elements present (diagrams, tables, charts, images)
- Overall structure

### 2. Transcription
Output using the `document_elements` array:

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique identifier (e.g., B1, N1) |
| `text` | string | The transcribed content |
| `type` | enum | TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, KEY_VALUE_PAIR |
| `relations` | array | Connections: `target_id` and `relation_type` (FLOWS_TO, VALUE_FOR) |

## Instructions
- **Text:** Transcribe all visible text
- **Diagrams:** Describe flow and connections using FLOWS_TO
- **Tables/Forms:** Link labels to values using VALUE_FOR
""",
    "JSON Structure": """
Task: Identify all meaningful blocks of content and extract the structural relationships between them.

JSON Schema: Output using the 'document_elements' array, where each object contains:
- id (string): Unique identifier (e.g., B1, N1)
- text (string): The transcribed content
- type (enum): TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR
- relations (array): Semantic connections with:
- target_id (string): Connected element's id
- relation_type (enum): FLOWS_TO, IS_LABEL_FOR, VALUE_FOR

Specific Instructions:
1. For diagrams: Use DIAGRAM_NODE for shapes, DIAGRAM_ARROW for lines. Link with FLOWS_TO.
2. For forms/tables: Use KEY_VALUE_PAIR. Link labels to values using VALUE_FOR.
""",
    "Custom": ""
}


@observe(name="streamlit-gemini-call", as_type="generation", capture_input=False, capture_output=False)
def call_gemini(input_prompt, ground_truth, model_id="gemini-2.0-flash", file_paths=None, generation_config=None, session_id="streamlit_session"):
    """Process files with Gemini and trace with Langfuse."""
    with propagate_attributes(
        user_id="streamlit_user",
        session_id=session_id,
        tags=["gemini", "streamlit", "document-analysis"],
        metadata={"source": "streamlit_app"},
        version="1.0.0",
    ):
        client = init_gemini_client()
        
        if file_paths is None:
            file_paths = []
        elif isinstance(file_paths, str):
            file_paths = [file_paths]
        
        uploaded_files = []
        
        try:
            for file_path in file_paths:
                uploaded_file = client.files.upload(file=file_path)
                uploaded_files.append(uploaded_file)
            
            contents = [input_prompt] + uploaded_files

            if generation_config:
                response = client.models.generate_content(
                    model=model_id,
                    contents=contents,
                    config=generation_config,
                )
            else:
                response = client.models.generate_content(
                    model=model_id,
                    contents=contents,
                )
            
            usage_meta = response.usage_metadata
            prompt_tokens = usage_meta.prompt_token_count or 0
            candidate_tokens = usage_meta.candidates_token_count or 0
            thought_tokens = usage_meta.thoughts_token_count or 0
            cached_tokens = usage_meta.cached_content_token_count or 0
            total_tokens = usage_meta.total_token_count or 0

            effective_output_tokens = candidate_tokens + thought_tokens

            langfuse.update_current_trace(
                input={
                    "prompt": input_prompt,
                    "files": [f.name for f in uploaded_files],
                    "file_count": len(uploaded_files)
                },
                output=response.text,
                metadata={
                    "ground_truth": ground_truth,
                    "model_id": model_id
                }
            )

            INPUT_PRICE_PER_TOKEN = 0.3 / 1000000
            OUTPUT_PRICE_PER_TOKEN = 2.5 / 1000000
            CACHING_PRICE_PER_TOKEN = 0.03 / 1000000

            input_cost = prompt_tokens * INPUT_PRICE_PER_TOKEN
            output_cost = effective_output_tokens * OUTPUT_PRICE_PER_TOKEN
            cache_read_input_cost = cached_tokens * CACHING_PRICE_PER_TOKEN
            total_cost = input_cost + output_cost + cache_read_input_cost
            
            langfuse.update_current_generation(
                cost_details={
                    "input": input_cost,
                    "cache_read_input_tokens": cache_read_input_cost,
                    "output": output_cost,
                    "total": total_cost,
                },
                usage_details={
                    "input": prompt_tokens,
                    "output": effective_output_tokens,
                    "cache_read_input_tokens": cached_tokens 
                },
            )

            usage_info = {
                "prompt_tokens": prompt_tokens,
                "output_tokens": effective_output_tokens,
                "cached_tokens": cached_tokens,
                "total_tokens": total_tokens,
                "total_cost": total_cost
            }

            return response.text, ground_truth, usage_info
            
        except Exception as e:
            raise e
        finally:
            for uploaded_file in uploaded_files:
                try:
                    client.files.delete(name=uploaded_file.name)
                except Exception:
                    pass


@observe(as_type="evaluator")
def evaluate_with_gemini(prediction, ground_truth, session_id="streamlit_session"):
    """Evaluate prediction against ground truth using Gemini."""
    eval_generation_config = types.GenerateContentConfig(
        temperature=0.0,
        top_p=0.9,
        top_k=40,
        max_output_tokens=2048,
        system_instruction="You are an expert evaluator for document analysis. Be thorough and fair in your assessment.",
        response_mime_type="application/json",
    )

    eval_prompt = f"""
    You are evaluating a document analysis output against the ground truth.
    
    Analyze both texts carefully and provide a comprehensive evaluation.
    
    Return a JSON object with these fields:
    {{
        "score": <float between 0 and 1, where 1 is perfect match>,
        "grade": <letter grade: A, B, C, D, or F>,
        "reason": <detailed explanation of the overall score>,
        "content_accuracy": {{
            "score": <float 0-1>,
            "details": <explanation of text/content accuracy>
        }},
        "structure_accuracy": {{
            "score": <float 0-1>,
            "details": <explanation of structural accuracy>
        }},
        "completeness": {{
            "score": <float 0-1>,
            "details": <explanation of how complete the analysis is>
        }},
        "matches": [<list of correctly identified elements>],
        "misses": [<list of missed or incorrect elements>],
        "extra": [<list of elements in prediction but not in ground truth>],
        "suggestions": [<list of improvement suggestions>]
    }}

    STRICT RULES:
    - Output ONLY valid JSON
    - Do NOT include backticks, markdown, or any text outside the JSON
    - Be objective and thorough
    - Consider partial matches

    === GROUND TRUTH ===
    {ground_truth}

    === PREDICTION ===
    {prediction}
    """

    with propagate_attributes(
        user_id="streamlit_user",
        session_id=session_id,
        tags=["gemini", "evaluation"],
    ):
        client = init_gemini_client()
        
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[eval_prompt],
            config=eval_generation_config,
        )
        
        raw_output = response.text
        clean_json = raw_output.replace("```json", "").replace("```", "").strip()

        try:
            result = json.loads(clean_json)
        except Exception as e:
            raise ValueError(f"Gemini did not return valid JSON: {clean_json}") from e

        # Ensure all expected fields exist with defaults
        result.setdefault("score", 0)
        result.setdefault("grade", "N/A")
        result.setdefault("reason", "No reason provided")
        result.setdefault("content_accuracy", {"score": 0, "details": "N/A"})
        result.setdefault("structure_accuracy", {"score": 0, "details": "N/A"})
        result.setdefault("completeness", {"score": 0, "details": "N/A"})
        result.setdefault("matches", [])
        result.setdefault("misses", [])
        result.setdefault("extra", [])
        result.setdefault("suggestions", [])

        langfuse.score_current_trace(
            name="evaluation_score",
            value=float(result["score"]),
            comment=result["reason"],
        )

        return result


def save_uploaded_file(uploaded_file):
    """Save uploaded file to a temporary location and return the path."""
    suffix = Path(uploaded_file.name).suffix
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        return tmp_file.name


def get_score_color(score):
    """Return color based on score."""
    if score >= 0.8:
        return "#4CAF50"  # Green
    elif score >= 0.6:
        return "#8BC34A"  # Light Green
    elif score >= 0.4:
        return "#FFC107"  # Amber
    elif score >= 0.2:
        return "#FF9800"  # Orange
    else:
        return "#F44336"  # Red


def get_grade_emoji(grade):
    """Return emoji based on grade."""
    grade_emojis = {
        "A": "üèÜ",
        "B": "üëç",
        "C": "üëå",
        "D": "‚ö†Ô∏è",
        "F": "‚ùå"
    }
    return grade_emojis.get(grade, "üìä")


def render_evaluation_results(result):
    """Render comprehensive evaluation results."""
    
    score = float(result["score"])
    grade = result.get("grade", "N/A")
    score_color = get_score_color(score)
    grade_emoji = get_grade_emoji(grade)
    
    # Custom CSS for evaluation display
    st.markdown("""
        <style>
        .eval-card {
            background: white;
            border-radius: 12px;
            padding: 1.5rem;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 1rem;
        }
        .score-circle {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            margin: 0 auto;
            color: white;
            font-weight: bold;
        }
        .metric-bar {
            height: 8px;
            border-radius: 4px;
            background: #e0e0e0;
            overflow: hidden;
            margin-top: 0.5rem;
        }
        .metric-fill {
            height: 100%;
            border-radius: 4px;
            transition: width 0.5s ease;
        }
        .tag {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.85rem;
            margin: 0.25rem;
        }
        .tag-success {
            background: #E8F5E9;
            color: #2E7D32;
            border: 1px solid #A5D6A7;
        }
        .tag-error {
            background: #FFEBEE;
            color: #C62828;
            border: 1px solid #EF9A9A;
        }
        .tag-warning {
            background: #FFF3E0;
            color: #E65100;
            border: 1px solid #FFCC80;
        }
        .tag-info {
            background: #E3F2FD;
            color: #1565C0;
            border: 1px solid #90CAF9;
        }
        </style>
    """, unsafe_allow_html=True)
    
    # ========== HEADER SECTION ==========
    st.markdown("## üìä Evaluation Results")
    st.markdown(f"*Evaluated on {datetime.now().strftime('%Y-%m-%d at %H:%M:%S')}*")
    
    st.divider()
    
    # ========== MAIN SCORE SECTION ==========
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        # Score Circle
        st.markdown(f"""
            <div style="text-align: center;">
                <div class="score-circle" style="background: linear-gradient(135deg, {score_color} 0%, {score_color}99 100%);">
                    <span style="font-size: 2.5rem;">{score:.0%}</span>
                    <span style="font-size: 0.9rem; opacity: 0.9;">Overall Score</span>
                </div>
                <div style="margin-top: 1rem;">
                    <span style="font-size: 3rem;">{grade_emoji}</span>
                    <h2 style="margin: 0; color: {score_color};">Grade: {grade}</h2>
                </div>
            </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # Reason/Summary
        st.markdown("### üìù Evaluation Summary")
        st.info(result["reason"])
        
        # Quick Stats
        st.markdown("### üìà Quick Stats")
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        with stat_col1:
            st.metric("‚úÖ Matches", len(result.get("matches", [])))
        with stat_col2:
            st.metric("‚ùå Misses", len(result.get("misses", [])))
        with stat_col3:
            st.metric("‚ûï Extra", len(result.get("extra", [])))
    
    with col3:
        # Progress indicators for sub-scores
        st.markdown("### üéØ Sub-Scores")
        
        content_score = result.get("content_accuracy", {}).get("score", 0)
        structure_score = result.get("structure_accuracy", {}).get("score", 0)
        completeness_score = result.get("completeness", {}).get("score", 0)
        
        # Content Accuracy
        st.markdown(f"**Content Accuracy:** {content_score:.0%}")
        st.progress(content_score)
        
        # Structure Accuracy
        st.markdown(f"**Structure Accuracy:** {structure_score:.0%}")
        st.progress(structure_score)
        
        # Completeness
        st.markdown(f"**Completeness:** {completeness_score:.0%}")
        st.progress(completeness_score)
    
    st.divider()
    
    # ========== DETAILED BREAKDOWN ==========
    st.markdown("## üîç Detailed Breakdown")
    
    detail_tab1, detail_tab2, detail_tab3 = st.tabs(["üìä Sub-Score Details", "‚úÖ‚ùå Matches & Misses", "üí° Suggestions"])
    
    with detail_tab1:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("### üìÑ Content Accuracy")
            content_info = result.get("content_accuracy", {})
            score_val = content_info.get("score", 0)
            st.markdown(f"""
                <div class="eval-card">
                    <h1 style="color: {get_score_color(score_val)}; margin: 0;">{score_val:.0%}</h1>
                    <p style="color: #666; margin-top: 1rem;">{content_info.get('details', 'N/A')}</p>
                </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("### üèóÔ∏è Structure Accuracy")
            structure_info = result.get("structure_accuracy", {})
            score_val = structure_info.get("score", 0)
            st.markdown(f"""
                <div class="eval-card">
                    <h1 style="color: {get_score_color(score_val)}; margin: 0;">{score_val:.0%}</h1>
                    <p style="color: #666; margin-top: 1rem;">{structure_info.get('details', 'N/A')}</p>
                </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("### üìã Completeness")
            completeness_info = result.get("completeness", {})
            score_val = completeness_info.get("score", 0)
            st.markdown(f"""
                <div class="eval-card">
                    <h1 style="color: {get_score_color(score_val)}; margin: 0;">{score_val:.0%}</h1>
                    <p style="color: #666; margin-top: 1rem;">{completeness_info.get('details', 'N/A')}</p>
                </div>
            """, unsafe_allow_html=True)
    
    with detail_tab2:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("### ‚úÖ Correct Matches")
            matches = result.get("matches", [])
            if matches:
                for match in matches:
                    st.markdown(f'<span class="tag tag-success">‚úì {match}</span>', unsafe_allow_html=True)
            else:
                st.info("No matches recorded")
        
        with col2:
            st.markdown("### ‚ùå Missed Elements")
            misses = result.get("misses", [])
            if misses:
                for miss in misses:
                    st.markdown(f'<span class="tag tag-error">‚úó {miss}</span>', unsafe_allow_html=True)
            else:
                st.success("No misses - great job!")
        
        with col3:
            st.markdown("### ‚ûï Extra Elements")
            extras = result.get("extra", [])
            if extras:
                for extra in extras:
                    st.markdown(f'<span class="tag tag-warning">+ {extra}</span>', unsafe_allow_html=True)
            else:
                st.info("No extra elements detected")
    
    with detail_tab3:
        st.markdown("### üí° Improvement Suggestions")
        suggestions = result.get("suggestions", [])
        if suggestions:
            for i, suggestion in enumerate(suggestions, 1):
                st.markdown(f"""
                    <div style="padding: 1rem; background: #F5F5F5; border-radius: 8px; margin-bottom: 0.5rem; border-left: 4px solid #2196F3;">
                        <strong>Suggestion {i}:</strong> {suggestion}
                    </div>
                """, unsafe_allow_html=True)
        else:
            st.success("No specific suggestions - the analysis looks good!")
    
    st.divider()
    
    # ========== SIDE BY SIDE COMPARISON ==========
    st.markdown("## üìë Side-by-Side Comparison")
    
    compare_col1, compare_col2 = st.columns(2)
    
    with compare_col1:
        st.markdown("### üéØ Ground Truth")
        with st.container(height=400):
            st.markdown(st.session_state.ground_truth)
    
    with compare_col2:
        st.markdown("### ü§ñ Prediction")
        with st.container(height=400):
            st.markdown(st.session_state.prediction)
    
    st.divider()
    
    # ========== EXPORT SECTION ==========
    st.markdown("## üì• Export Results")
    
    # Generate report
    report = f"""# Evaluation Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Overall Score: {score:.1%} (Grade: {grade})

### Summary
{result['reason']}

---

## Sub-Scores

| Metric | Score | Details |
|--------|-------|---------|
| Content Accuracy | {result.get('content_accuracy', {}).get('score', 0):.0%} | {result.get('content_accuracy', {}).get('details', 'N/A')} |
| Structure Accuracy | {result.get('structure_accuracy', {}).get('score', 0):.0%} | {result.get('structure_accuracy', {}).get('details', 'N/A')} |
| Completeness | {result.get('completeness', {}).get('score', 0):.0%} | {result.get('completeness', {}).get('details', 'N/A')} |

---

## Matches ({len(result.get('matches', []))})
{chr(10).join(['- ' + m for m in result.get('matches', [])]) or 'None'}

## Misses ({len(result.get('misses', []))})
{chr(10).join(['- ' + m for m in result.get('misses', [])]) or 'None'}

## Extra Elements ({len(result.get('extra', []))})
{chr(10).join(['- ' + m for m in result.get('extra', [])]) or 'None'}

---

## Suggestions
{chr(10).join(['- ' + s for s in result.get('suggestions', [])]) or 'None'}

---

## Ground Truth
{st.session_state.ground_truth}

## Prediction
{st.session_state.prediction}

"""
    
    export_col1, export_col2, export_col3 = st.columns(3)
    
    with export_col1:
        st.download_button(
            label="üìÑ Download Report (Markdown)",
            data=report,
            file_name=f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown",
            use_container_width=True
        )
    
    with export_col2:
        st.download_button(
            label="üìä Download Results (JSON)",
            data=json.dumps(result, indent=2),
            file_name=f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True
        )
    
    with export_col3:
        # Create CSV-like summary
        csv_data = f"""Metric,Score,Details
Overall,{score},{result['reason'].replace(',', ';')}
Content Accuracy,{result.get('content_accuracy', {}).get('score', 0)},{result.get('content_accuracy', {}).get('details', 'N/A').replace(',', ';')}
Structure Accuracy,{result.get('structure_accuracy', {}).get('score', 0)},{result.get('structure_accuracy', {}).get('details', 'N/A').replace(',', ';')}
Completeness,{result.get('completeness', {}).get('score', 0)},{result.get('completeness', {}).get('details', 'N/A').replace(',', ';')}
"""
        st.download_button(
            label="üìà Download Summary (CSV)",
            data=csv_data,
            file_name=f"evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

# @observe(name="pipelinex", as_type="chain")
def main():
    # Custom CSS
    st.markdown("""
        <style>
        .main-header {
            font-size: 2.5rem;
            font-weight: bold;
            color: #1E88E5;
            margin-bottom: 1rem;
        }
        .section-header {
            font-size: 1.5rem;
            font-weight: 600;
            color: #424242;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
        }
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            padding-left: 20px;
            padding-right: 20px;
        }
        </style>
    """, unsafe_allow_html=True)

    # Header
    st.markdown('<p class="main-header">üìÑ Document Analyzer with Gemini</p>', unsafe_allow_html=True)
    st.markdown("Upload a document, analyze it with Gemini, and evaluate against ground truth.")
    
    st.divider()

    # Sidebar Configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        model_id = st.selectbox(
            "ü§ñ Select Model",
            ["gemini-3-pro-preview", "gemini-2.0-flash", "gemini-2.5-flash"],
            index=0,
        )
        
        st.divider()
        
        st.subheader("Generation Settings")
        
        temperature = st.slider("Temperature", 0.0, 2.0, 0.0, 0.1)
        max_tokens = st.number_input("Max Output Tokens", 256, 32768, 20000, 256)
        
        enable_thinking = st.checkbox("Enable Thinking Mode", False)
        thinking_budget = 0
        if enable_thinking:
            thinking_budget = st.slider("Thinking Budget", 1024, 16384, 4096, 1024)
        
        st.divider()
        
        st.subheader("Prompt Template")
        prompt_template = st.selectbox("Select Template", list(PROMPTS.keys()), 0)

    # Initialize session state
    if "prediction" not in st.session_state:
        st.session_state.prediction = None
    if "usage_info" not in st.session_state:
        st.session_state.usage_info = None
    if "ground_truth" not in st.session_state:
        st.session_state.ground_truth = None
    if "evaluation_result" not in st.session_state:
        st.session_state.evaluation_result = None

    # Main content tabs
    tab1, tab2, tab3, tab4 = st.tabs(["üì§ Upload & Analyze", "üìù Results", "‚úÖ Evaluate", "üìä Evaluation Results"])

    # Tab 1: Upload and Analyze
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown('<p class="section-header">Upload Document</p>', unsafe_allow_html=True)
            
            uploaded_doc = st.file_uploader(
                "Upload PDF or Image",
                type=["pdf", "png", "jpg", "jpeg", "webp", "gif"],
            )
            
            if uploaded_doc:
                st.success(f"‚úÖ Uploaded: {uploaded_doc.name}")
                if uploaded_doc.type.startswith("image/"):
                    st.image(uploaded_doc, caption="Document Preview", use_container_width=True)
                else:
                    st.info(f"üìé PDF file: {uploaded_doc.name} ({uploaded_doc.size / 1024:.1f} KB)")
        
        with col2:
            st.markdown('<p class="section-header">Prompt Configuration</p>', unsafe_allow_html=True)
            
            if prompt_template == "Custom":
                instruction_prompt = st.text_area("Custom Prompt", height=300, placeholder="Enter your custom prompt...")
            else:
                instruction_prompt = st.text_area("Prompt (editable)", value=PROMPTS[prompt_template], height=300)
        
        st.divider()
        
        col_btn1, col_btn2, col_btn3 = st.columns([1, 2, 1])
        with col_btn2:
            analyze_button = st.button(
                "üöÄ Analyze Document",
                type="primary",
                use_container_width=True,
                disabled=not uploaded_doc or not instruction_prompt
            )
        
        if analyze_button and uploaded_doc and instruction_prompt:
            with st.spinner("üîÑ Processing document with Gemini..."):
                try:
                    temp_path = save_uploaded_file(uploaded_doc)
                    
                    gen_config_params = {
                        "temperature": temperature,
                        "max_output_tokens": max_tokens,
                        "system_instruction": "You are an expert Document Analyzer.",
                    }
                    
                    if enable_thinking:
                        gen_config_params["thinking_config"] = types.ThinkingConfig(
                            thinking_budget=thinking_budget,
                            include_thoughts=True,
                        )
                    
                    generation_config = types.GenerateContentConfig(**gen_config_params)
                    
                    prediction, _, usage_info = call_gemini(
                        input_prompt=instruction_prompt,
                        ground_truth=None,
                        model_id=model_id,
                        file_paths=temp_path,
                        generation_config=generation_config,
                        session_id=f"streamlit_{uploaded_doc.name}"
                    )
                    
                    st.session_state.prediction = prediction
                    st.session_state.usage_info = usage_info
                    st.session_state.evaluation_result = None
                    
                    os.unlink(temp_path)
                    langfuse.flush()
                    
                    st.success("‚úÖ Analysis complete! Check the Results tab.")
                    st.balloons()
                    
                except Exception as e:
                    st.error(f"‚ùå Error during analysis: {str(e)}")

    # Tab 2: Results
    with tab2:
        if st.session_state.prediction:
            st.markdown('<p class="section-header">Analysis Results</p>', unsafe_allow_html=True)
            
            if st.session_state.usage_info:
                usage = st.session_state.usage_info
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Input Tokens", f"{usage['prompt_tokens']:,}")
                with col2:
                    st.metric("Output Tokens", f"{usage['output_tokens']:,}")
                with col3:
                    st.metric("Total Tokens", f"{usage['total_tokens']:,}")
                with col4:
                    st.metric("Est. Cost", f"${usage['total_cost']:.6f}")
            
            st.divider()
            
            display_mode = st.radio("Display Mode", ["Rendered Markdown", "Raw Text"], horizontal=True)
            
            if display_mode == "Rendered Markdown":
                st.markdown(st.session_state.prediction)
            else:
                st.code(st.session_state.prediction, language="markdown")
            
            st.divider()
            col1, col2 = st.columns([1, 1])
            with col1:
                st.download_button(
                    label="üì• Download as Markdown",
                    data=st.session_state.prediction,
                    file_name="analysis_result.md",
                    mime="text/markdown"
                )
            with col2:
                st.download_button(
                    label="üì• Download as Text",
                    data=st.session_state.prediction,
                    file_name="analysis_result.txt",
                    mime="text/plain"
                )
        else:
            st.info("üìå No results yet. Upload a document and analyze it first.")

    # Tab 3: Evaluate
    with tab3:
        st.markdown('<p class="section-header">Upload Ground Truth</p>', unsafe_allow_html=True)
        
        if not st.session_state.prediction:
            st.warning("‚ö†Ô∏è Please analyze a document first before evaluation.")
        else:
            col1, col2 = st.columns([1, 1])
            
            with col1:
                ground_truth_file = st.file_uploader(
                    "Upload Ground Truth (Markdown or Text)",
                    type=["md", "txt", "text"],
                )
                
                if ground_truth_file:
                    ground_truth_content = ground_truth_file.read().decode("utf-8")
                    st.session_state.ground_truth = ground_truth_content
                    st.success(f"‚úÖ Loaded: {ground_truth_file.name}")
                
                st.markdown("**Or paste ground truth directly:**")
                pasted_ground_truth = st.text_area(
                    "Paste Ground Truth",
                    height=200,
                    placeholder="Paste your ground truth text here..."
                )
                
                if pasted_ground_truth:
                    st.session_state.ground_truth = pasted_ground_truth
            
            with col2:
                if st.session_state.ground_truth:
                    st.markdown("**Ground Truth Preview:**")
                    with st.expander("View Ground Truth", expanded=True):
                        st.markdown(st.session_state.ground_truth)
            
            st.divider()
            
            if st.session_state.ground_truth:
                col_btn1, col_btn2, col_btn3 = st.columns([1, 2, 1])
                with col_btn2:
                    evaluate_button = st.button(
                        "üéØ Run Evaluation",
                        type="primary",
                        use_container_width=True
                    )
                
                if evaluate_button:
                    with st.spinner("üîÑ Evaluating with Gemini..."):
                        try:
                            eval_result = evaluate_with_gemini(
                                prediction=st.session_state.prediction,
                                ground_truth=st.session_state.ground_truth,
                                session_id="streamlit_evaluation"
                            )
                            
                            st.session_state.evaluation_result = eval_result
                            langfuse.flush()
                            
                            st.success("‚úÖ Evaluation complete! Check the 'Evaluation Results' tab.")
                            
                            langfuse.flush()
                            
                        except Exception as e:
                            langfuse.flush()
                            st.error(f"‚ùå Evaluation error: {str(e)}")

    # Tab 4: Evaluation Results (NEW DEDICATED TAB)
    with tab4:
        if st.session_state.evaluation_result:
            render_evaluation_results(st.session_state.evaluation_result)
        else:
            st.info("üìå No evaluation results yet. Run an evaluation first from the 'Evaluate' tab.")
            
            # Show what's needed
            st.markdown("### üìã Checklist")
            
            check1 = "‚úÖ" if st.session_state.prediction else "‚¨ú"
            check2 = "‚úÖ" if st.session_state.ground_truth else "‚¨ú"
            check3 = "‚úÖ" if st.session_state.evaluation_result else "‚¨ú"
            
            st.markdown(f"""
            {check1} **Step 1:** Analyze a document (Upload & Analyze tab)
            
            {check2} **Step 2:** Upload ground truth (Evaluate tab)
            
            {check3} **Step 3:** Run evaluation (Evaluate tab)
            """)

    # Footer
    st.divider()
    st.markdown(
        """
        <div style="text-align: center; color: #888; font-size: 0.8rem;">
            Built with Streamlit ‚Ä¢ Powered by Google Gemini ‚Ä¢ Traced with Langfuse
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
