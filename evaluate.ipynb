{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3becf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
      "Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl (94 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 281.8 kB/s  0:00:05\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ------------- -------------------------- 1/3 [Levenshtein]\n",
      "   ---------------------------------------- 3/3 [python-Levenshtein]\n",
      "\n",
      "Successfully installed Levenshtein-0.27.3 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc97b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (1.0.4)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-google-genai) (1.54.0)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-1.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-google-genai) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (0.4.32)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading langchain_core-1.1.2-py3-none-any.whl (475 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Installing collected packages: filetype, uuid-utils, langchain-core, langchain-google-genai\n",
      "\n",
      "  Attempting uninstall: langchain-core\n",
      "\n",
      "    Found existing installation: langchain-core 1.0.4\n",
      "\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "    Uninstalling langchain-core-1.0.4:\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-1.0.4\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   ------------------------------ --------- 3/4 [langchain-google-genai]\n",
      "   ---------------------------------------- 4/4 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 langchain-core-1.1.2 langchain-google-genai-4.0.0 uuid-utils-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.2.12 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 1.1.2 which is incompatible.\n",
      "langchain-together 0.3.1 requires langchain-core<0.4.0,>=0.3.29, but you have langchain-core 1.1.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38743504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langfuse\n",
      "  Downloading langfuse-3.10.5-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from langfuse)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (0.28.1)\n",
      "Requirement already satisfied: openai>=0.27.8 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.109.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.38.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (2.32.5)\n",
      "Collecting wrapt<2.0,>=1.14 (from langfuse)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (4.15.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.72.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (6.32.1)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3,>=2->langfuse) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3,>=2->langfuse) (2.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from tqdm>4->openai>=0.27.8->langfuse) (0.4.6)\n",
      "Downloading langfuse-3.10.5-py3-none-any.whl (398 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: wrapt, opentelemetry-proto, backoff, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, langfuse\n",
      "\n",
      "  Attempting uninstall: wrapt\n",
      "\n",
      "    Found existing installation: wrapt 2.0.1\n",
      "\n",
      "    Uninstalling wrapt-2.0.1:\n",
      "\n",
      "      Successfully uninstalled wrapt-2.0.1\n",
      "\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "    Found existing installation: opentelemetry-api 1.38.0\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "    Uninstalling opentelemetry-api-1.38.0:\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "      Successfully uninstalled opentelemetry-api-1.38.0\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "   ----------------- ---------------------- 4/9 [opentelemetry-api]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   -------------------------- ------------- 6/9 [opentelemetry-sdk]\n",
      "   ------------------------ ------ 7/9 [opentelemetry-exporter-otlp-proto-http]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ---------------------------------------- 9/9 [langfuse]\n",
      "\n",
      "Successfully installed backoff-2.2.1 langfuse-3.10.5 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-http-1.39.0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 wrapt-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "label-studio 1.21.0 requires pyarrow<19.0.0,>=18.1.0, but you have pyarrow 22.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OCR EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä LAYOUT-AWARE METRICS (Order Matters)\n",
      "------------------------------------------------------------\n",
      "  CER (Character Error Rate):     0.2529\n",
      "  CER Accuracy:                   0.7471\n",
      "  WER (Word Error Rate):          0.2909\n",
      "  WER Accuracy:                   0.7091\n",
      "\n",
      "üìù CONTENT-ONLY METRICS (Order Ignored)\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.7707\n",
      "  Recall:                         0.7333\n",
      "  F1 Score:                       0.7516\n",
      "  Correct Words:                  121\n",
      "  Missing Words:                  44\n",
      "  Extra Words:                    36\n",
      "\n",
      "üî§ CHARACTER-LEVEL CONTENT METRICS\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.9725\n",
      "  Recall:                         0.9567\n",
      "  F1 Score:                       0.9645\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Levenshtein\n",
    "# from collections import Counter\n",
    "\n",
    "# def calculate_layout_aware_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Calculates Character Error Rate (CER) and Word Error Rate (WER) \n",
    "#     using Levenshtein distance. These metrics are LAYOUT-AWARE \n",
    "#     (order matters).\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip()\n",
    "#     ext_clean = extracted_text.strip()\n",
    "    \n",
    "#     # Character Error Rate (CER)\n",
    "#     char_distance = Levenshtein.distance(gt_clean, ext_clean)\n",
    "#     char_length = len(gt_clean)\n",
    "#     cer = char_distance / char_length if char_length > 0 else 0.0\n",
    "    \n",
    "#     # Word Error Rate (WER)\n",
    "#     gt_words = gt_clean.split()\n",
    "#     ext_words = ext_clean.split()\n",
    "#     word_distance = Levenshtein.distance(gt_words, ext_words)\n",
    "#     word_length = len(gt_words)\n",
    "#     wer = word_distance / word_length if word_length > 0 else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"Layout_Aware_CER\": round(cer, 4),\n",
    "#         \"Layout_Aware_WER\": round(wer, 4),\n",
    "#         \"Layout_Aware_CER_Accuracy\": round(1 - cer, 4),\n",
    "#         \"Layout_Aware_WER_Accuracy\": round(1 - wer, 4),\n",
    "#     }\n",
    "\n",
    "# def calculate_content_only_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Compares only text content with word frequency, completely \n",
    "#     LAYOUT-AGNOSTIC (order doesn't matter). Best for evaluating \n",
    "#     OCR text extraction quality independent of layout.\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip().lower()\n",
    "#     ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "#     gt_counter = Counter(gt_clean.split())\n",
    "#     ext_counter = Counter(ext_clean.split())\n",
    "    \n",
    "#     # Words that appear with correct frequency\n",
    "#     correct_counts = sum((gt_counter & ext_counter).values())\n",
    "#     total_gt_words = sum(gt_counter.values())\n",
    "#     total_ext_words = sum(ext_counter.values())\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     recall = correct_counts / total_gt_words if total_gt_words > 0 else 0.0\n",
    "#     precision = correct_counts / total_ext_words if total_ext_words > 0 else 0.0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "#     # Additional insights\n",
    "#     missing_words = total_gt_words - correct_counts\n",
    "#     extra_words = total_ext_words - correct_counts\n",
    "    \n",
    "#     return {\n",
    "#         \"Content_Precision\": round(precision, 4),\n",
    "#         \"Content_Recall\": round(recall, 4),\n",
    "#         \"Content_F1_Score\": round(f1, 4),\n",
    "#         \"Correct_Words\": correct_counts,\n",
    "#         \"Missing_Words\": missing_words,\n",
    "#         \"Extra_Words\": extra_words,\n",
    "#         \"Total_GT_Words\": total_gt_words,\n",
    "#         \"Total_Extracted_Words\": total_ext_words,\n",
    "#     }\n",
    "\n",
    "# def calculate_character_content_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Character-level content comparison (layout-agnostic).\n",
    "#     Useful for languages with complex word boundaries like Sinhala/Tamil.\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip().lower()\n",
    "#     ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "#     # Remove all whitespace for pure character comparison\n",
    "#     gt_chars = Counter(gt_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "#     ext_chars = Counter(ext_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "    \n",
    "#     correct_chars = sum((gt_chars & ext_chars).values())\n",
    "#     total_gt_chars = sum(gt_chars.values())\n",
    "#     total_ext_chars = sum(ext_chars.values())\n",
    "    \n",
    "#     recall = correct_chars / total_gt_chars if total_gt_chars > 0 else 0.0\n",
    "#     precision = correct_chars / total_ext_chars if total_ext_chars > 0 else 0.0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"Character_Content_Precision\": round(precision, 4),\n",
    "#         \"Character_Content_Recall\": round(recall, 4),\n",
    "#         \"Character_Content_F1\": round(f1, 4),\n",
    "#     }\n",
    "\n",
    "# def evaluate_ocr_from_files(ground_truth_file: str, extracted_text_file: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Main function to evaluate OCR accuracy from two text files.\n",
    "    \n",
    "#     Args:\n",
    "#         ground_truth_file: Path to the ground truth text file\n",
    "#         extracted_text_file: Path to the extracted/predicted text file\n",
    "        \n",
    "#     Returns:\n",
    "#         Dictionary containing all evaluation metrics\n",
    "#     \"\"\"\n",
    "#     # Read files\n",
    "#     try:\n",
    "#         with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "#             ground_truth = f.read()\n",
    "#         with open(extracted_text_file, 'r', encoding='utf-8') as f:\n",
    "#             extracted_text = f.read()\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"Error: File not found - {e}\")\n",
    "#         return {}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading files: {e}\")\n",
    "#         return {}\n",
    "    \n",
    "#     # Calculate all metrics\n",
    "#     results = {}\n",
    "    \n",
    "#     # Layout-aware metrics (traditional CER/WER)\n",
    "#     layout_metrics = calculate_layout_aware_metrics(ground_truth, extracted_text)\n",
    "#     results.update(layout_metrics)\n",
    "    \n",
    "#     # Content-only metrics (layout-agnostic)\n",
    "#     content_metrics = calculate_content_only_metrics(ground_truth, extracted_text)\n",
    "#     results.update(content_metrics)\n",
    "    \n",
    "#     # Character-level content metrics\n",
    "#     char_metrics = calculate_character_content_metrics(ground_truth, extracted_text)\n",
    "#     results.update(char_metrics)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def print_results(results: dict):\n",
    "#     \"\"\"Pretty print the evaluation results.\"\"\"\n",
    "#     if not results:\n",
    "#         print(\"No results to display.\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"OCR EVALUATION RESULTS\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     print(\"\\nüìä LAYOUT-AWARE METRICS (Order Matters)\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  CER (Character Error Rate):     {results['Layout_Aware_CER']}\")\n",
    "#     print(f\"  CER Accuracy:                   {results['Layout_Aware_CER_Accuracy']}\")\n",
    "#     print(f\"  WER (Word Error Rate):          {results['Layout_Aware_WER']}\")\n",
    "#     print(f\"  WER Accuracy:                   {results['Layout_Aware_WER_Accuracy']}\")\n",
    "    \n",
    "#     print(\"\\nüìù CONTENT-ONLY METRICS (Order Ignored)\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  Precision:                      {results['Content_Precision']}\")\n",
    "#     print(f\"  Recall:                         {results['Content_Recall']}\")\n",
    "#     print(f\"  F1 Score:                       {results['Content_F1_Score']}\")\n",
    "#     print(f\"  Correct Words:                  {results['Correct_Words']}\")\n",
    "#     print(f\"  Missing Words:                  {results['Missing_Words']}\")\n",
    "#     print(f\"  Extra Words:                    {results['Extra_Words']}\")\n",
    "    \n",
    "#     print(\"\\nüî§ CHARACTER-LEVEL CONTENT METRICS\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  Precision:                      {results['Character_Content_Precision']}\")\n",
    "#     print(f\"  Recall:                         {results['Character_Content_Recall']}\")\n",
    "#     print(f\"  F1 Score:                       {results['Character_Content_F1']}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# # === USAGE EXAMPLE ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Specify your file paths\n",
    "#     ground_truth_file = \"original.txt\"\n",
    "#     extracted_text_file = \"predicted.txt\"\n",
    "    \n",
    "#     # Run evaluation\n",
    "#     results = evaluate_ocr_from_files(ground_truth_file, extracted_text_file)\n",
    "    \n",
    "#     # Display results\n",
    "#     print_results(results)\n",
    "    \n",
    "#     # Optionally save results to JSON\n",
    "#     # import json\n",
    "#     # with open('evaluation_results.json', 'w') as f:\n",
    "#     #     json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0741a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class MarkdownEvaluator:\n",
    "    def __init__(self, header_threshold=0.8):\n",
    "        self.header_threshold = header_threshold\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalizes text for comparison:\n",
    "        1. Removes markdown formatting (*, _, `) but KEEPS pipes | for tables.\n",
    "        2. Normalizes whitespace.\n",
    "        3. Lowers case (optional, strictness depends on use case).\n",
    "        \"\"\"\n",
    "        # Remove bold/italic/code markers\n",
    "        text = re.sub(r'[*_`]', '', text) \n",
    "        # Normalize whitespace (tabs/newlines -> single space)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def parse_markdown(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses text into sections. \n",
    "        If no headers (#) are found, treats the whole text as a 'Document' section.\n",
    "        \"\"\"\n",
    "        # Regex for standard Markdown headers (# Header)\n",
    "        pattern = re.compile(r'(^|\\n)(#+)\\s*(.*?)(?=\\n#|\\Z)', re.DOTALL)\n",
    "        sections = {}\n",
    "        \n",
    "        matches = list(pattern.finditer(text))\n",
    "        \n",
    "        # FAILSAFE: If no headers found, treat entire text as body content\n",
    "        if not matches:\n",
    "            sections['Whole Document'] = {\n",
    "                'title': 'Whole Document',\n",
    "                'content': text.strip()\n",
    "            }\n",
    "            return sections\n",
    "\n",
    "        # If headers exist, parse normally\n",
    "        if matches[0].start() > 0:\n",
    "            preamble = text[:matches[0].start()].strip()\n",
    "            if preamble:\n",
    "                sections['PREAMBLE'] = {'title': 'PREAMBLE', 'content': preamble}\n",
    "\n",
    "        for match in matches:\n",
    "            hashes, title, content = match.group(2), match.group(3), match.group(0)\n",
    "            # Remove the header line itself from the content to avoid duplication\n",
    "            content_only = content.replace(f\"{hashes} {title}\", \"\", 1).strip()\n",
    "            \n",
    "            sections[title.strip()] = {\n",
    "                'title': title.strip(),\n",
    "                'content': content_only\n",
    "            }\n",
    "            \n",
    "        return sections\n",
    "\n",
    "    def get_diff_highlight(self, a: str, b: str) -> str:\n",
    "        \"\"\"Helper to show exactly where characters differ.\"\"\"\n",
    "        s = SequenceMatcher(None, a, b)\n",
    "        diff_out = []\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            if tag == 'replace':\n",
    "                diff_out.append(f\"MISMATCH: '{a[i1:i2]}' vs '{b[j1:j2]}'\")\n",
    "            elif tag == 'delete':\n",
    "                diff_out.append(f\"MISSING in Pred: '{a[i1:i2]}'\")\n",
    "            elif tag == 'insert':\n",
    "                diff_out.append(f\"EXTRA in Pred: '{b[j1:j2]}'\")\n",
    "        return \" | \".join(diff_out)\n",
    "\n",
    "    def evaluate(self, gt_text: str, pred_text: str) -> dict:\n",
    "        gt_sections = self.parse_markdown(gt_text)\n",
    "        pred_sections = self.parse_markdown(pred_text)\n",
    "        \n",
    "        results = {'matches': [], 'score_sum': 0, 'count': 0}\n",
    "        \n",
    "        # Combine Title + Content for comparison to catch everything\n",
    "        def get_full_text(sec):\n",
    "            # If it's the \"Whole Document\" fallback, just return content\n",
    "            if sec['title'] == 'Whole Document':\n",
    "                return sec['content']\n",
    "            return f\"{sec['title']} {sec['content']}\"\n",
    "\n",
    "        matched_pred_keys = set()\n",
    "\n",
    "        for gt_key, gt_data in gt_sections.items():\n",
    "            best_match = None\n",
    "            best_score = 0.0\n",
    "            \n",
    "            gt_full_clean = self.clean_text(get_full_text(gt_data))\n",
    "            \n",
    "            # Find best matching section in Pred\n",
    "            for pred_key, pred_data in pred_sections.items():\n",
    "                if pred_key in matched_pred_keys: continue\n",
    "                \n",
    "                # Compare fuzzy headers, OR if we are in \"Whole Document\" mode, compare full text\n",
    "                if gt_key == \"Whole Document\" or pred_key == \"Whole Document\":\n",
    "                     # If parsing failed, we force a comparison of the body\n",
    "                    header_sim = 1.0 \n",
    "                else:\n",
    "                    header_sim = SequenceMatcher(None, gt_key, pred_key).ratio()\n",
    "\n",
    "                if header_sim > self.header_threshold:\n",
    "                    pred_full_clean = self.clean_text(get_full_text(pred_data))\n",
    "                    content_sim = SequenceMatcher(None, gt_full_clean, pred_full_clean).ratio()\n",
    "                    \n",
    "                    if content_sim > best_score:\n",
    "                        best_score = content_sim\n",
    "                        best_match = pred_key\n",
    "\n",
    "            if best_match:\n",
    "                matched_pred_keys.add(best_match)\n",
    "                \n",
    "                # Get raw texts for diffing\n",
    "                gt_raw = self.clean_text(get_full_text(gt_data))\n",
    "                pred_raw = self.clean_text(get_full_text(pred_sections[best_match]))\n",
    "                \n",
    "                print(f\"Cleaned ground truth: {gt_raw}\")\n",
    "                print(f\"Cleaned prediction  : {pred_raw}\")\n",
    "\n",
    "                diff_notes = \"\"\n",
    "                if best_score < 1.0:\n",
    "                    diff_notes = self.get_diff_highlight(gt_raw, pred_raw)\n",
    "\n",
    "                results['matches'].append({\n",
    "                    'section': gt_key,\n",
    "                    'score': best_score,\n",
    "                    'diff': diff_notes\n",
    "                })\n",
    "                results['score_sum'] += best_score\n",
    "                results['count'] += 1\n",
    "            else:\n",
    "                # Missing section penalty\n",
    "                results['matches'].append({'section': gt_key, 'score': 0.0, 'diff': \"Section Missing\"})\n",
    "                results['count'] += 1\n",
    "\n",
    "        results['final_score'] = results['score_sum'] / results['count'] if results['count'] > 0 else 0\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc37eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned ground truth: 03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂±‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫. 04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫. 05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É ‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±. ‡∂Ö) 1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í. 4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö. (‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
      "Cleaned prediction  : 03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂± ‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫. 04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫. 05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É ‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±. (‡∂Ö) 1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í. 4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö. (‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
      "Overall Score: 0.9989\n",
      "----------------------------------------\n",
      "Section: Whole Document\n",
      "Score:   0.9989\n",
      "Errors:  EXTRA in Pred: ' ' | EXTRA in Pred: '('\n"
     ]
    }
   ],
   "source": [
    "gt = \"\"\n",
    "pred = \"\"\n",
    "with open(\"o.txt\", \"r\") as original_file:\n",
    "    for line in original_file.readlines():\n",
    "        gt = gt + \"\\n\" + line\n",
    "\n",
    "with open(\"p.txt\", \"r\") as pred_file:\n",
    "    for line in pred_file.readlines():\n",
    "        pred = pred + \"\\n\" + line\n",
    "\n",
    "evaluator = MarkdownEvaluator()\n",
    "metrics = evaluator.evaluate(gt, pred)\n",
    "\n",
    "print(f\"Overall Score: {metrics['final_score']:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "for m in metrics['matches']:\n",
    "    print(f\"Section: {m['section']}\")\n",
    "    print(f\"Score:   {m['score']:.4f}\")\n",
    "    if m['diff']:\n",
    "        print(f\"Errors:  {m['diff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1622cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (1.54.0)\n",
      "Requirement already satisfied: openai in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (1.109.1)\n",
      "Requirement already satisfied: langfuse in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (3.10.5)\n",
      "Collecting openinference-instrumentation-google-genai\n",
      "  Downloading openinference_instrumentation_google_genai-0.1.8-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (2.11.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (2.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.39.0)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (24.2)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.17.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.39.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (6.32.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse) (0.60b0)\n",
      "Collecting openinference-instrumentation>=0.1.17 (from openinference-instrumentation-google-genai)\n",
      "  Downloading openinference_instrumentation-0.1.42-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting openinference-semantic-conventions (from openinference-instrumentation-google-genai)\n",
      "  Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-instrumentation (from openinference-instrumentation-google-genai)\n",
      "  Downloading opentelemetry_instrumentation-0.60b0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openinference_instrumentation_google_genai-0.1.8-py3-none-any.whl (23 kB)\n",
      "Downloading openinference_instrumentation-0.1.42-py3-none-any.whl (30 kB)\n",
      "Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl (10 kB)\n",
      "Downloading opentelemetry_instrumentation-0.60b0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: openinference-semantic-conventions, opentelemetry-instrumentation, openinference-instrumentation, openinference-instrumentation-google-genai\n",
      "\n",
      "   -------------------- ------------------- 2/4 [openinference-instrumentation]\n",
      "   --------------------------- 4/4 [openinference-instrumentation-google-genai]\n",
      "\n",
      "Successfully installed openinference-instrumentation-0.1.42 openinference-instrumentation-google-genai-0.1.8 openinference-semantic-conventions-0.1.25 opentelemetry-instrumentation-0.60b0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai openai langfuse openinference-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys ‚úã\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03939acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor\n",
    " \n",
    "GoogleGenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: 12.pdf...\n",
      "File uploaded successfully. URI: https://generativelanguage.googleapis.com/v1beta/files/8z67lmfjpidv\n",
      "\n",
      "Requesting transcription and translation...\n",
      "--- Transcription/Translation Result ---\n",
      "Please find the transcription and translation of the provided image below. The OCR output was highly inaccurate and contained multiple languages; therefore, a manual transcription from the image was performed.\n",
      "\n",
      "---\n",
      "\n",
      "## Page 1 Transcription and Translation\n",
      "\n",
      "*(Note: This page contains two recipes, one in the left column and one in the right column.)*\n",
      "\n",
      "**Left Column: ‡∂Ö‡∂π ‡∂¥‡∑î‡∂©‡∑í‡∂Ç (Amba Pu·∏çin) - Mango Pudding**\n",
      "\n",
      "**‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫: (Ava≈õya dravya:) - Required ingredients:**\n",
      "\n",
      "1.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1¬Ω ‡∂ö‡∑ä ‡∂ã‡∂´‡∑î‡∑Ä‡∂≠‡∑î‡∂ª‡∑ô‡∂±‡∑ä ‡∂≠‡∂∏‡∑ä‡∂∂‡∑è ‡∂¥‡∑ô‡∂ª‡∑è ‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (K≈çpi k≈çppha 1¬Ω k u·πáuvaturen thambƒÅ perƒÅ ganna.)\n",
      "    Boil 1¬Ω cups of coffee in hot water and strain.\n",
      "2.  ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∂ö‡∑Ñ ‡∂∏‡∂Ø 2‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∂∏ ‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (Biththara kaha mada 2k wenama ganna.)\n",
      "    Take 2 egg yolks separately.\n",
      "3.  ‡∂†‡∑ì‡∑É‡∑ä ‡∂ö‡∑ê‡∂∂‡∂Ω‡∑í 9‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´ ‡∑É‡∑í‡∑Ñ‡∑í‡∂±‡∑ä‡∑Ä ‡∂ö‡∂¥‡∑è‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (Chƒ´s k√§bali 9k pama·πáa sihinwa kapƒÅganna.)\n",
      "    Cut about 9 pieces of cheese thinly.\n",
      "    *(There's a small, faint drawing below this, with text that looks like \"‡∂≠‡∑ö‡∂ß ‡∑É‡∑ì‡∂±‡∑í ‡∂ö‡∑î‡∂©‡∑î\" (Tƒì·π≠a sƒ´ni ku·∏çu) - Sugar powder for tea)*\n",
      "\n",
      "---\n",
      "\n",
      "**Right Column: ‡∂ö‡∑ù‡∂¥‡∑í ‡∂¥‡∑î‡∂©‡∑í‡∂Ç (K≈çpi Pu·∏çin) - Coffee Pudding**\n",
      "\n",
      "**‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫: (Ava≈õya dravya:) - Required ingredients:**\n",
      "\n",
      "1.  ‡∑É‡∑ì‡∂±‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/4‡∂ö‡∑ä\n",
      "    (Sƒ´ni k≈çppha 1/4k)\n",
      "    1/4 cup of sugar\n",
      "2.  ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/4‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/4k)\n",
      "    1/4 cup of milk\n",
      "3.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑î‡∂©‡∑î ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 1/2‡∂ö‡∑ä\n",
      "    (K≈çpi ku·∏çu tƒì h√§ndi 1/2k)\n",
      "    1/2 teaspoon of coffee powder\n",
      "4.  ‡∂¢‡∑ô‡∂Ω‡∂ß‡∑í‡∂±‡∑ä ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Jela·π≠in tƒì h√§ndi 2k)\n",
      "    2 teaspoons of gelatin\n",
      "5.  ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∂ö‡∑Ñ ‡∂∏‡∂Ø 2‡∂ö‡∑ä\n",
      "    (Biththara kaha mada 2k)\n",
      "    2 egg yolks\n",
      "6.  ‡∑É‡∑ì‡∂±‡∑í ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 3‡∂ö‡∑ä\n",
      "    (Sƒ´ni mƒìsa h√§ndi 3k)\n",
      "    3 tablespoons of sugar\n",
      "7.  ‡∂∂‡∂ß‡∂ª‡∑ä ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Ba·π≠ar mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of butter\n",
      "8.  ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/2k)\n",
      "    1/2 cup of milk\n",
      "9.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (K≈çpi k≈çppha 1/2k)\n",
      "    1/2 cup of coffee\n",
      "10. ‡∂∂‡∑í‡∑É‡∑ä‡∂ö‡∂ß‡∑ä ‡∂ö‡∑î‡∂©‡∑î ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Bis ka·π≠ ku·∏çu mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of biscuit crumbs\n",
      "11. ‡∑Ä‡∑ê‡∂±‡∑í‡∂Ω‡∑è ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 1‡∂ö‡∑ä\n",
      "    (WanilƒÅ tƒì h√§ndi 1k)\n",
      "    1 teaspoon of vanilla\n",
      "12. ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∑É‡∑î‡∂Ø‡∑î ‡∂∏‡∂Ø 2‡∂ö‡∑ä\n",
      "    (Biththara sudu mada 2k)\n",
      "    2 egg whites\n",
      "13. ‡∂Ö‡∂∫‡∑í‡∑É‡∑í‡∂Ç ‡∑É‡∑ì‡∂±‡∑í ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Aisin sƒ´ni mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of icing sugar\n",
      "14. ‡∂¢‡∑ô‡∂Ω‡∂ß‡∑í‡∂±‡∑ä ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Jela·π≠in k≈çppha 1/2k)\n",
      "    1/2 cup of gelatin\n",
      "15. ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/2k)\n",
      "    1/2 cup of milk\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "*   ‡∂∏‡∑ö ‡∑É‡∑í‡∂∫‡∂Ω‡∑î ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫ ‡∑Ñ‡∑ú‡∂≥‡∑í‡∂±‡∑ä ‡∂∏‡∑í‡∑Å‡∑ä‚Äç‡∂ª ‡∂ö‡∂ª.\n",
      "    (Mƒì siyalu dravya ho≈àdin mishra kara.)\n",
      "    Mix all these ingredients well.\n",
      "*   ‡∂ß‡∑ä‚Äç‡∂ª‡∑ö ‡∂ë‡∂ö‡∂ö‡∂ß ‡∂Ø‡∂∏‡∑è.\n",
      "    (·π¨rƒì eka·π≠a damƒÅ.)\n",
      "    Put into a tray.\n",
      "*   ‡∂¥‡∑ê‡∂∫ 3-4‡∂ö‡∑ä ‡∑Å‡∑ì‡∂≠‡∂ö‡∂ª‡∂´‡∂∫‡∑ö ‡∂≠‡∂∂‡∂±‡∑ä‡∂±.\n",
      "    (Paya 3-4k ≈õƒ´thakara·πáayƒì thabanna.)\n",
      "    Leave in the refrigerator for 3-4 hours.\n",
      "*   ‡∂ö‡∑ë‡∂∏‡∂ß ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª (‡∂ú‡∂±‡∑ä‡∂±)\n",
      "    (K√§ma·π≠a genƒ´ma·π≠a perÃåa (ganna))\n",
      "    Before eating (serve).\n",
      "\n",
      "---\n",
      "\n",
      "## Page 2 Transcription and Translation\n",
      "\n",
      "*(This page contains general information, possibly related to an electricity service or organization.)*\n",
      "\n",
      "1.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∂∑‡∑è‡∂≠‡∑í‡∂ö‡∑è ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫\n",
      "    (Mƒ´rigama BhƒÅthikƒÅ Viduliya)\n",
      "    Mirigama Bhathika Electricity\n",
      "2.  ‡∑É‡∑ö‡∑Ä‡∂ö ‡∑É‡∂∏‡∑í‡∂≠‡∑í‡∂∫,\n",
      "    (Sƒìwaka Samithiya,)\n",
      "    Employees' Society,\n",
      "3.  ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ô‡∂± ‡∂≠‡∑ô‡∂Ω‡∑ä\n",
      "    (Viduliya labƒÅdena thel)\n",
      "    Oil that provides electricity\n",
      "4.  ‡∂∂‡∂Ω‡∂ú‡∂±‡∑ä‡∑Ä‡∑è ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (BalaganwƒÅ thibƒì.)\n",
      "    Has been empowered / is supplied.\n",
      "5.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∂∑‡∑è‡∂≠‡∑í‡∂ö‡∑è ‡∂¥‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∂ß\n",
      "    (Mƒ´rigama BhƒÅthikƒÅ pradƒì≈õaya·π≠a)\n",
      "    To the Mirigama Bhathika area\n",
      "6.  ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (Viduliya labƒÅ dƒ´ thibƒì.)\n",
      "    Electricity has been provided.\n",
      "7.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∑É‡∑ö‡∑Ä‡∂ö ‡∑É‡∂∏‡∑í‡∂≠‡∑í‡∂∫\n",
      "    (Mƒ´rigama Sƒìwaka Samithiya)\n",
      "    Mirigama Employees' Society\n",
      "8.  ‡∂¢‡∂∫‡∂ú‡∑ä‚Äç‡∂ª‡∑Ñ‡∂´‡∂∫\n",
      "    (Jayagraha·πáaya)\n",
      "    Victory\n",
      "9.  ‡∂Ω‡∂∂‡∑è ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (LabƒÅ thibƒì.)\n",
      "    Has achieved.\n",
      "--------------------------------------\n",
      "\n",
      "Deleting uploaded file: files/8z67lmfjpidv...\n",
      "File deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# --- Configuration ---\n",
    "IMAGE_PATH = \"12.pdf\"\n",
    "PROMPT = \"give me the transcription.\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Uploading file: {IMAGE_PATH}...\")\n",
    "try:\n",
    "    uploaded_file = client.files.upload(file=IMAGE_PATH)\n",
    "    print(f\"File uploaded successfully. URI: {uploaded_file.uri}\")\n",
    "\n",
    "    print(\"\\nRequesting transcription and translation...\")\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[PROMPT, uploaded_file]\n",
    "    )\n",
    "\n",
    "    print(\"--- Transcription/Translation Result ---\")\n",
    "    print(response.text)\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {IMAGE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'uploaded_file' in locals():\n",
    "        print(f\"\\nDeleting uploaded file: {uploaded_file.name}...\")\n",
    "        client.files.delete(name=uploaded_file.name)\n",
    "        print(\"File deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4037f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: Generated Image December 09, 2025 - 4_41PM.jpeg...\n",
      "‚úì File uploaded: files/t91pdsk3e4lc (URI: https://generativelanguage.googleapis.com/v1beta/files/t91pdsk3e4lc)\n",
      "\n",
      "Processing 1 file(s) with prompt...\n",
      "\n",
      "--- Response ---\n",
      "```json\n",
      "{\n",
      "  \"document_elements\": [\n",
      "    {\n",
      "      \"id\": \"N_No\",\n",
      "      \"text\": \"No\",\n",
      "      \"type\": \"KEY_VALUE_PAIR\",\n",
      "      \"bbox\": [87, 6, 90, 9],\n",
      "      \"relations\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"N_Date\",\n",
      "      \"text\": \"Date\",\n",
      "      \"type\": \"KEY_VALUE_PAIR\",\n",
      "      \"bbox\": [87, 10, 90, 13],\n",
      "      \"relations\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B1\",\n",
      "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 2\",\n",
      "      \"type\": \"TITLE\",\n",
      "      \"bbox\": [16, 20, 30, 24],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B2\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B2\",\n",
      "      \"text\": \"- ‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (MRI - Magnetic Resonance Imaging Machine)\",\n",
      "      \"type\": \"LIST\",\n",
      "      \"bbox\": [13, 27, 92, 35],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B3\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B3\",\n",
      "      \"text\": \"‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä ‡∑Ä‡∂Ω ‡∑É‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∂ª‡∑ñ‡∂¥ ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∂ú‡∂ª‡∑ä‡∂∑‡∂∫‡∑ö ‡∂ª‡∑ñ‡∂¥ ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö‡∂Ø‡∑ì‡∂≠‡∑ä, ‡∂ª‡∑ù‡∂ú ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö‡∂Ø‡∑ì‡∂≠‡∑ä ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂ª‡∑ñ‡∂¥ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\n",
      "      \"type\": \"PARAGRAPH\",\n",
      "      \"bbox\": [14, 38, 98, 48],\n",
      "      \"relations\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B4\",\n",
      "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 3\",\n",
      "      \"type\": \"TITLE\",\n",
      "      \"bbox\": [16, 51, 30, 55],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B5\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B5\",\n",
      "      \"text\": \"- ‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑î‡∂≠‡∑ä ‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ö‡∂õ‡∑ì‡∂∫ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (ECG - Electrocardiogram Machine)\",\n",
      "      \"type\": \"LIST\",\n",
      "      \"bbox\": [13, 58, 95, 65],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B6\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B6\",\n",
      "      \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫ ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è, ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∫‡∑ú‡∂Ø‡∑è ‡∂ú‡∑ê‡∂±‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∂ª‡∑ù‡∂ú‡∂∫‡∂ö‡∑ä ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä, ‡∂∏‡∑ô‡∂∏ ‡∂ª‡∑ö‡∂õ‡∑ì‡∂∫ ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫‡∂ß ‡∂Ö‡∂±‡∑î‡∑Ä ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∑Ä‡∂± ‡∂ª‡∑ù‡∂ú ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä ‡∂∏‡∑ô‡∂∫ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\n",
      "      \"type\": \"PARAGRAPH\",\n",
      "      \"bbox\": [14, 68, 98, 78],\n",
      "      \"relations\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B7\",\n",
      "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 4\",\n",
      "      \"type\": \"TITLE\",\n",
      "      \"bbox\": [16, 81, 30, 85],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B8\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B8\",\n",
      "      \"text\": \"- ‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ù‡∂ú ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (Cardiac Screening Machine)\",\n",
      "      \"type\": \"LIST\",\n",
      "      \"bbox\": [13, 88, 95, 95],\n",
      "      \"relations\": [\n",
      "        {\n",
      "          \"target_id\": \"B9\",\n",
      "          \"relation_type\": \"FLOWS_TO\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"B9\",\n",
      "      \"text\": \"‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è‡∂ö‡∑è‡∂ª‡∑ì‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∑è ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂±‡∑è‡∂Ω ‡∂Ö‡∑Ä‡∑Ñ‡∑í‡∂ª ‡∑Ä‡∑ì ‡∂≠‡∑í‡∂∂‡∑ö‡∂Ø, ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂≠‡∑è ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä, ‡∂â‡∂ö‡∑ä‡∂∏‡∂±‡∑ä ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∫‡∑ú‡∂∏‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä ‡∂∏‡∑ô‡∂∫ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\n",
      "      \"type\": \"PARAGRAPH\",\n",
      "      \"bbox\": [14, 97, 98, 100],\n",
      "      \"relations\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "----------------\n",
      "Deleting uploaded file: files/t91pdsk3e4lc...\n",
      "‚úì File deleted: files/t91pdsk3e4lc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"document_elements\": [\\n    {\\n      \"id\": \"N_No\",\\n      \"text\": \"No\",\\n      \"type\": \"KEY_VALUE_PAIR\",\\n      \"bbox\": [87, 6, 90, 9],\\n      \"relations\": []\\n    },\\n    {\\n      \"id\": \"N_Date\",\\n      \"text\": \"Date\",\\n      \"type\": \"KEY_VALUE_PAIR\",\\n      \"bbox\": [87, 10, 90, 13],\\n      \"relations\": []\\n    },\\n    {\\n      \"id\": \"B1\",\\n      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 2\",\\n      \"type\": \"TITLE\",\\n      \"bbox\": [16, 20, 30, 24],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B2\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B2\",\\n      \"text\": \"- ‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ (MRI - Magnetic Resonance Imaging Machine)\",\\n      \"type\": \"LIST\",\\n      \"bbox\": [13, 27, 92, 35],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B3\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B3\",\\n      \"text\": \"‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂∑‡∑ä\\u200d‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä ‡∑Ä‡∂Ω ‡∑É‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∂ª‡∑ñ‡∂¥ ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ ‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∂ú‡∂ª‡∑ä‡∂∑‡∂∫‡∑ö ‡∂ª‡∑ñ‡∂¥ ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö‡∂Ø‡∑ì‡∂≠‡∑ä, ‡∂ª‡∑ù‡∂ú ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö‡∂Ø‡∑ì‡∂≠‡∑ä ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂ª‡∑ñ‡∂¥ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\\n      \"type\": \"PARAGRAPH\",\\n      \"bbox\": [14, 38, 98, 48],\\n      \"relations\": []\\n    },\\n    {\\n      \"id\": \"B4\",\\n      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 3\",\\n      \"type\": \"TITLE\",\\n      \"bbox\": [16, 51, 30, 55],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B5\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B5\",\\n      \"text\": \"- ‡∑Ä‡∑í‡∂Ø‡∑ä\\u200d‡∂∫‡∑î‡∂≠‡∑ä ‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ö‡∂õ‡∑ì‡∂∫ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ (ECG - Electrocardiogram Machine)\",\\n      \"type\": \"LIST\",\\n      \"bbox\": [13, 58, 95, 65],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B6\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B6\",\\n      \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫ ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è, ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ ‡∂∫‡∑ú‡∂Ø‡∑è ‡∂ú‡∑ê‡∂±‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∂ª‡∑ù‡∂ú‡∂∫‡∂ö‡∑ä ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä, ‡∂∏‡∑ô‡∂∏ ‡∂ª‡∑ö‡∂õ‡∑ì‡∂∫ ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫‡∂ß ‡∂Ö‡∂±‡∑î‡∑Ä ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∑Ä‡∂± ‡∂ª‡∑ù‡∂ú ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä ‡∂∏‡∑ô‡∂∫ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\\n      \"type\": \"PARAGRAPH\",\\n      \"bbox\": [14, 68, 98, 78],\\n      \"relations\": []\\n    },\\n    {\\n      \"id\": \"B7\",\\n      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 4\",\\n      \"type\": \"TITLE\",\\n      \"bbox\": [16, 81, 30, 85],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B8\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B8\",\\n      \"text\": \"- ‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ù‡∂ú ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ (Cardiac Screening Machine)\",\\n      \"type\": \"LIST\",\\n      \"bbox\": [13, 88, 95, 95],\\n      \"relations\": [\\n        {\\n          \"target_id\": \"B9\",\\n          \"relation_type\": \"FLOWS_TO\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"B9\",\\n      \"text\": \"‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ö‡∑ä\\u200d‡∂ª‡∑í‡∂∫‡∑è‡∂ö‡∑è‡∂ª‡∑ì‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∑è ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä\\u200d‡∂ª‡∂∫ ‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂±‡∑è‡∂Ω ‡∂Ö‡∑Ä‡∑Ñ‡∑í‡∂ª ‡∑Ä‡∑ì ‡∂≠‡∑í‡∂∂‡∑ö‡∂Ø, ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂≠‡∑è ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä, ‡∂â‡∂ö‡∑ä‡∂∏‡∂±‡∑ä ‡∂¥‡∑ä\\u200d‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∫‡∑ú‡∂∏‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß‡∂≠‡∑ä ‡∂∏‡∑ô‡∂∫ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\\n      \"type\": \"PARAGRAPH\",\\n      \"bbox\": [14, 97, 98, 100],\\n      \"relations\": []\\n    }\\n  ]\\n}\\n```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import observe, propagate_attributes, Langfuse\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "\n",
    "@observe(name=\"gemini-call\", as_type=\"generation\", capture_input=True, capture_output=True)\n",
    "def my_llm_pipeline(input, ground_truth, model_id=\"gemini-2.0-flash\", file_paths=None):\n",
    "    \"\"\"\n",
    "    Process multiple files with Gemini and trace with Langfuse.\n",
    "    \n",
    "    Args:\n",
    "        input: Text prompt/instruction\n",
    "        model_id: Gemini model to use\n",
    "        file_paths: List of file paths or single file path (string)\n",
    "    \"\"\"\n",
    "    with propagate_attributes(\n",
    "        user_id=\"eshanj\",\n",
    "        session_id=\"session_x\",\n",
    "        tags=[\"gemini\", \"eshan's-trace\", \"multi-file\"],\n",
    "        metadata={\"email\": \"eshan@fonixedu.com\"},\n",
    "        version=\"1.0.0\",\n",
    "    ):\n",
    "        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        \n",
    "        # Handle both single file and multiple files\n",
    "        if file_paths is None:\n",
    "            file_paths = []\n",
    "        elif isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        uploaded_files = []\n",
    "        \n",
    "        try:\n",
    "            # Upload all files\n",
    "            for file_path in file_paths:\n",
    "                print(f\"Uploading file: {file_path}...\")\n",
    "                uploaded_file = client.files.upload(file=file_path)\n",
    "                uploaded_files.append(uploaded_file)\n",
    "                print(f\"‚úì File uploaded: {uploaded_file.name} (URI: {uploaded_file.uri})\")\n",
    "            \n",
    "            # Build content array: [prompt, file1, file2, file3, ...]\n",
    "            contents = [input] + uploaded_files\n",
    "            \n",
    "            print(f\"\\nProcessing {len(uploaded_files)} file(s) with prompt...\")\n",
    "            response = client.models.generate_content(\n",
    "                model=model_id,\n",
    "                contents=contents,\n",
    "            )\n",
    "            \n",
    "            print(\"\\n--- Response ---\")\n",
    "            print(response.text)\n",
    "            print(\"----------------\")\n",
    "            \n",
    "            # Update trace with metadata about files\n",
    "            langfuse.update_current_trace(\n",
    "                input={\n",
    "                    \"prompt\": input,\n",
    "                    \"files\": [f.name for f in uploaded_files],\n",
    "                    \"file_count\": len(uploaded_files)\n",
    "                },\n",
    "                output=response.text,\n",
    "                metadata={\"ground_truth\": ground_truth},\n",
    "            )\n",
    "            \n",
    "            return response.text\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"ERROR: File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Clean up all uploaded files\n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    print(f\"Deleting uploaded file: {uploaded_file.name}...\")\n",
    "                    client.files.delete(name=uploaded_file.name)\n",
    "                    print(f\"‚úì File deleted: {uploaded_file.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {uploaded_file.name}: {e}\")\n",
    "\n",
    "# The JSON prompt should be formatted as a string with proper escapes\n",
    "SYSTEM_INSTRUCTION_PROMPT = \"\"\"\n",
    "You are an expert Document Layout and Diagram Analyzer. Your task is to process the provided handwritten document image, including any diagrams, tables, or complex layouts. Your entire response MUST be a single JSON object.\n",
    "\n",
    "Task: Identify all meaningful blocks of content and extract the structural relationships between them.\n",
    "\n",
    "JSON Schema: Output the prediction using the 'document_elements' array, where each object contains:\n",
    "- id (string): A unique identifier (e.g., B1, N_Start).\n",
    "- text (string): The transcribed content.\n",
    "- type (enum): The element's function. Use only: TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR.\n",
    "- bbox (array of 4 integers): Normalized coordinates [xmin, ymin, xmax, ymax]. All values MUST be integers between 0 and 100.\n",
    "- relations (array of objects): A list of semantic connections.\n",
    "\n",
    "Relations Schema (Inside relations):\n",
    "- target_id (string): The id of the element it connects to.\n",
    "- relation_type (enum): The connection type. Use: FLOWS_TO, IS_LABEL_FOR, VALUE_FOR.\n",
    "\n",
    "Specific Instructions:\n",
    "1. For diagrams, use DIAGRAM_NODE for shapes and DIAGRAM_ARROW for lines. Use FLOWS_TO to link the source node to the target node.\n",
    "2. For forms/tables, use KEY_VALUE_PAIR. If a value is separated from its label, link them using VALUE_FOR.\n",
    "\"\"\"\n",
    "\n",
    "GROUND_TRUTH = \"\"\"```json\n",
    "{\n",
    "  \"document_elements\": [\n",
    "    {\n",
    "      \"id\": \"T1\",\n",
    "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 2\",\n",
    "      \"type\": \"TITLE\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        9,\n",
    "        30,\n",
    "        12\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"H1\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"H1\",\n",
    "      \"text\": \"‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø ‡∂∏‡∑ñ‡∂ª‡∑ä‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (MRI - Magnetic Resonance Imaging Machine)\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        14,\n",
    "        92,\n",
    "        20\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"P1\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"P1\",\n",
    "      \"text\": \"‡∂ª‡∑ö‡∂©‡∑í‡∂∫‡∑ù ‡∂≠‡∂ª‡∂Ç‡∂ú ‡∑É‡∑Ñ ‡∂¥‡∑ä‚Äç‡∂ª‡∂∂‡∂Ω ‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø (‡∂Ø‡∑ô‡∑Å‡∑í‡∂ö) ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä‡∑Ä‡∂Ω ‡∑É‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∂ª‡∑ñ‡∂¥ ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏ ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∂ª‡∑ù‡∂ú ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∂±‡∑í‡∂ª‡∑ä‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∂Ø ‡∂∏‡∑ô‡∂∏ ‡∂ª‡∑ñ‡∂¥ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        23,\n",
    "        99,\n",
    "        36\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"T2\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"T2\",\n",
    "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 3\",\n",
    "      \"type\": \"TITLE\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        39,\n",
    "        29,\n",
    "        42\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"H2\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"H2\",\n",
    "      \"text\": \"‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑î‡∂≠‡∑ä ‡∂≠‡∂±‡∑ä‡∂≠‡∑î ‡∂ª‡∑ö‡∂õ‡∑í‡∂∫ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (ECG - Electrocardiogram Machine)\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        44,\n",
    "        99,\n",
    "        50\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"P2\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"P2\",\n",
    "      \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫ ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∫‡∑ú‡∂Ø‡∑è ‡∂ú‡∑ê‡∂±‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∑É‡∑í‡∂ß ‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö‡∑î‡∂≠‡∑ä ‡∂â‡∂±‡∑ä‡∂Ø‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∂∫‡∂±‡∑ä ‡∑Ä‡∑ô‡∂≠ ‡∂ª‡∑î‡∂∞‡∑í‡∂ª‡∂∫ ‡∑É‡∑ê‡∂¥‡∂∫‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑î‡∂≠‡∑ä ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫‡∂ß ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂±‡∑í‡∂¥‡∂Ø‡∑Ä‡∑ô‡∂± ‡∂≠‡∂ª‡∂Ç‡∂ú ‡∂¥‡∑ä‚Äç‡∂ª‡∑É‡∑ä‡∂≠‡∑è‡∂ª‡∑í‡∂ö ‡∂ö‡∂©‡∂Ø‡∑è‡∑É‡∑í‡∂∫‡∂ö ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∑Ä‡∑ì‡∂∏ ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂Ø‡∑ì ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö.\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        53,\n",
    "        99,\n",
    "        67\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"T3\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"T3\",\n",
    "      \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 4\",\n",
    "      \"type\": \"TITLE\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        69,\n",
    "        31,\n",
    "        72\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"H3\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"H3\",\n",
    "      \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ù‡∂ú ‡∂±‡∑í‡∂ª‡∑ä ‡∂ú‡∂±‡∑ä‡∑Ä‡∑ì‡∂∏‡∑ö ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (Cardiac Screening Machine)\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        74,\n",
    "        97,\n",
    "        80\n",
    "      ],\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"target_id\": \"P3\",\n",
    "          \"relation_type\": \"FLOWS_TO\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"P3\",\n",
    "      \"text\": \"‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è‡∂ö‡∑è‡∂ª‡∑ì‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂¥‡∂ª‡∑í‡∂ú‡∂´‡∂ö ‡∂≠‡∑í‡∂ª‡∂∫‡∂ö ‡∂Ø‡∑ê‡∂ö‡∑ä‡∑Ä‡∑ì‡∂∏ ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂±‡∑è‡∂Ω ‡∑É‡∑í‡∑Ñ‡∑í‡∂±‡∑ä ‡∑Ä‡∑ì‡∂∏ ‡∑Ä‡∑ê‡∂±‡∑í ‡∑Ä‡∑í‡∑Ä‡∑í‡∂∞ ‡∂Ü‡∑É‡∑è‡∂Ø‡∂± ‡∂≠‡∂≠‡∑ä‡∂≠‡∑ä‡∑Ä‡∂∫‡∂±‡∑ä ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∑Ä‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∫‡∑ú‡∂∏‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂∏‡∑ö ‡∂±‡∑í‡∑É‡∑è ‡∂¥‡∑Ñ‡∑É‡∑î ‡∑Ä‡∑ö.\",\n",
    "      \"type\": \"PARAGRAPH\",\n",
    "      \"bbox\": [\n",
    "        12,\n",
    "        83,\n",
    "        98,\n",
    "        96\n",
    "      ],\n",
    "      \"relations\": []\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "my_llm_pipeline(SYSTEM_INSTRUCTION_PROMPT, GROUND_TRUTH, model_id=\"gemini-2.5-flash\", file_paths=\"Generated Image December 09, 2025 - 4_41PM.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f807c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
