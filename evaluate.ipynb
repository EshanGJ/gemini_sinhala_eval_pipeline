{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3becf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
      "Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl (94 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 281.8 kB/s  0:00:05\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ------------- -------------------------- 1/3 [Levenshtein]\n",
      "   ---------------------------------------- 3/3 [python-Levenshtein]\n",
      "\n",
      "Successfully installed Levenshtein-0.27.3 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc97b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (1.0.4)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-google-genai) (1.54.0)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-1.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-google-genai) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (0.4.32)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading langchain_core-1.1.2-py3-none-any.whl (475 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Installing collected packages: filetype, uuid-utils, langchain-core, langchain-google-genai\n",
      "\n",
      "  Attempting uninstall: langchain-core\n",
      "\n",
      "    Found existing installation: langchain-core 1.0.4\n",
      "\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "    Uninstalling langchain-core-1.0.4:\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-1.0.4\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   -------------------- ------------------- 2/4 [langchain-core]\n",
      "   ------------------------------ --------- 3/4 [langchain-google-genai]\n",
      "   ---------------------------------------- 4/4 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 langchain-core-1.1.2 langchain-google-genai-4.0.0 uuid-utils-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.2.12 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 1.1.2 which is incompatible.\n",
      "langchain-together 0.3.1 requires langchain-core<0.4.0,>=0.3.29, but you have langchain-core 1.1.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38743504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langfuse\n",
      "  Downloading langfuse-3.10.5-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from langfuse)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (0.28.1)\n",
      "Requirement already satisfied: openai>=0.27.8 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.109.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (1.38.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from langfuse) (2.32.5)\n",
      "Collecting wrapt<2.0,>=1.14 (from langfuse)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (4.15.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.72.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (6.32.1)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3,>=2->langfuse) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from requests<3,>=2->langfuse) (2.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from openai>=0.27.8->langfuse) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\eng\\lib\\site-packages (from tqdm>4->openai>=0.27.8->langfuse) (0.4.6)\n",
      "Downloading langfuse-3.10.5-py3-none-any.whl (398 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: wrapt, opentelemetry-proto, backoff, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, langfuse\n",
      "\n",
      "  Attempting uninstall: wrapt\n",
      "\n",
      "    Found existing installation: wrapt 2.0.1\n",
      "\n",
      "    Uninstalling wrapt-2.0.1:\n",
      "\n",
      "      Successfully uninstalled wrapt-2.0.1\n",
      "\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "    Found existing installation: opentelemetry-api 1.38.0\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "    Uninstalling opentelemetry-api-1.38.0:\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "      Successfully uninstalled opentelemetry-api-1.38.0\n",
      "   ---- ----------------------------------- 1/9 [opentelemetry-proto]\n",
      "   ----------------- ---------------------- 4/9 [opentelemetry-api]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   ------------------- --------------- 5/9 [opentelemetry-semantic-conventions]\n",
      "   -------------------------- ------------- 6/9 [opentelemetry-sdk]\n",
      "   ------------------------ ------ 7/9 [opentelemetry-exporter-otlp-proto-http]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ----------------------------------- ---- 8/9 [langfuse]\n",
      "   ---------------------------------------- 9/9 [langfuse]\n",
      "\n",
      "Successfully installed backoff-2.2.1 langfuse-3.10.5 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-http-1.39.0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 wrapt-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "label-studio 1.21.0 requires pyarrow<19.0.0,>=18.1.0, but you have pyarrow 22.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd64d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OCR EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä LAYOUT-AWARE METRICS (Order Matters)\n",
      "------------------------------------------------------------\n",
      "  CER (Character Error Rate):     0.2529\n",
      "  CER Accuracy:                   0.7471\n",
      "  WER (Word Error Rate):          0.2909\n",
      "  WER Accuracy:                   0.7091\n",
      "\n",
      "üìù CONTENT-ONLY METRICS (Order Ignored)\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.7707\n",
      "  Recall:                         0.7333\n",
      "  F1 Score:                       0.7516\n",
      "  Correct Words:                  121\n",
      "  Missing Words:                  44\n",
      "  Extra Words:                    36\n",
      "\n",
      "üî§ CHARACTER-LEVEL CONTENT METRICS\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.9725\n",
      "  Recall:                         0.9567\n",
      "  F1 Score:                       0.9645\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Levenshtein\n",
    "# from collections import Counter\n",
    "\n",
    "# def calculate_layout_aware_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Calculates Character Error Rate (CER) and Word Error Rate (WER) \n",
    "#     using Levenshtein distance. These metrics are LAYOUT-AWARE \n",
    "#     (order matters).\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip()\n",
    "#     ext_clean = extracted_text.strip()\n",
    "    \n",
    "#     # Character Error Rate (CER)\n",
    "#     char_distance = Levenshtein.distance(gt_clean, ext_clean)\n",
    "#     char_length = len(gt_clean)\n",
    "#     cer = char_distance / char_length if char_length > 0 else 0.0\n",
    "    \n",
    "#     # Word Error Rate (WER)\n",
    "#     gt_words = gt_clean.split()\n",
    "#     ext_words = ext_clean.split()\n",
    "#     word_distance = Levenshtein.distance(gt_words, ext_words)\n",
    "#     word_length = len(gt_words)\n",
    "#     wer = word_distance / word_length if word_length > 0 else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"Layout_Aware_CER\": round(cer, 4),\n",
    "#         \"Layout_Aware_WER\": round(wer, 4),\n",
    "#         \"Layout_Aware_CER_Accuracy\": round(1 - cer, 4),\n",
    "#         \"Layout_Aware_WER_Accuracy\": round(1 - wer, 4),\n",
    "#     }\n",
    "\n",
    "# def calculate_content_only_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Compares only text content with word frequency, completely \n",
    "#     LAYOUT-AGNOSTIC (order doesn't matter). Best for evaluating \n",
    "#     OCR text extraction quality independent of layout.\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip().lower()\n",
    "#     ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "#     gt_counter = Counter(gt_clean.split())\n",
    "#     ext_counter = Counter(ext_clean.split())\n",
    "    \n",
    "#     # Words that appear with correct frequency\n",
    "#     correct_counts = sum((gt_counter & ext_counter).values())\n",
    "#     total_gt_words = sum(gt_counter.values())\n",
    "#     total_ext_words = sum(ext_counter.values())\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     recall = correct_counts / total_gt_words if total_gt_words > 0 else 0.0\n",
    "#     precision = correct_counts / total_ext_words if total_ext_words > 0 else 0.0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "#     # Additional insights\n",
    "#     missing_words = total_gt_words - correct_counts\n",
    "#     extra_words = total_ext_words - correct_counts\n",
    "    \n",
    "#     return {\n",
    "#         \"Content_Precision\": round(precision, 4),\n",
    "#         \"Content_Recall\": round(recall, 4),\n",
    "#         \"Content_F1_Score\": round(f1, 4),\n",
    "#         \"Correct_Words\": correct_counts,\n",
    "#         \"Missing_Words\": missing_words,\n",
    "#         \"Extra_Words\": extra_words,\n",
    "#         \"Total_GT_Words\": total_gt_words,\n",
    "#         \"Total_Extracted_Words\": total_ext_words,\n",
    "#     }\n",
    "\n",
    "# def calculate_character_content_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Character-level content comparison (layout-agnostic).\n",
    "#     Useful for languages with complex word boundaries like Sinhala/Tamil.\n",
    "#     \"\"\"\n",
    "#     gt_clean = ground_truth.strip().lower()\n",
    "#     ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "#     # Remove all whitespace for pure character comparison\n",
    "#     gt_chars = Counter(gt_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "#     ext_chars = Counter(ext_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "    \n",
    "#     correct_chars = sum((gt_chars & ext_chars).values())\n",
    "#     total_gt_chars = sum(gt_chars.values())\n",
    "#     total_ext_chars = sum(ext_chars.values())\n",
    "    \n",
    "#     recall = correct_chars / total_gt_chars if total_gt_chars > 0 else 0.0\n",
    "#     precision = correct_chars / total_ext_chars if total_ext_chars > 0 else 0.0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"Character_Content_Precision\": round(precision, 4),\n",
    "#         \"Character_Content_Recall\": round(recall, 4),\n",
    "#         \"Character_Content_F1\": round(f1, 4),\n",
    "#     }\n",
    "\n",
    "# def evaluate_ocr_from_files(ground_truth_file: str, extracted_text_file: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Main function to evaluate OCR accuracy from two text files.\n",
    "    \n",
    "#     Args:\n",
    "#         ground_truth_file: Path to the ground truth text file\n",
    "#         extracted_text_file: Path to the extracted/predicted text file\n",
    "        \n",
    "#     Returns:\n",
    "#         Dictionary containing all evaluation metrics\n",
    "#     \"\"\"\n",
    "#     # Read files\n",
    "#     try:\n",
    "#         with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "#             ground_truth = f.read()\n",
    "#         with open(extracted_text_file, 'r', encoding='utf-8') as f:\n",
    "#             extracted_text = f.read()\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"Error: File not found - {e}\")\n",
    "#         return {}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading files: {e}\")\n",
    "#         return {}\n",
    "    \n",
    "#     # Calculate all metrics\n",
    "#     results = {}\n",
    "    \n",
    "#     # Layout-aware metrics (traditional CER/WER)\n",
    "#     layout_metrics = calculate_layout_aware_metrics(ground_truth, extracted_text)\n",
    "#     results.update(layout_metrics)\n",
    "    \n",
    "#     # Content-only metrics (layout-agnostic)\n",
    "#     content_metrics = calculate_content_only_metrics(ground_truth, extracted_text)\n",
    "#     results.update(content_metrics)\n",
    "    \n",
    "#     # Character-level content metrics\n",
    "#     char_metrics = calculate_character_content_metrics(ground_truth, extracted_text)\n",
    "#     results.update(char_metrics)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def print_results(results: dict):\n",
    "#     \"\"\"Pretty print the evaluation results.\"\"\"\n",
    "#     if not results:\n",
    "#         print(\"No results to display.\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"OCR EVALUATION RESULTS\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     print(\"\\nüìä LAYOUT-AWARE METRICS (Order Matters)\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  CER (Character Error Rate):     {results['Layout_Aware_CER']}\")\n",
    "#     print(f\"  CER Accuracy:                   {results['Layout_Aware_CER_Accuracy']}\")\n",
    "#     print(f\"  WER (Word Error Rate):          {results['Layout_Aware_WER']}\")\n",
    "#     print(f\"  WER Accuracy:                   {results['Layout_Aware_WER_Accuracy']}\")\n",
    "    \n",
    "#     print(\"\\nüìù CONTENT-ONLY METRICS (Order Ignored)\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  Precision:                      {results['Content_Precision']}\")\n",
    "#     print(f\"  Recall:                         {results['Content_Recall']}\")\n",
    "#     print(f\"  F1 Score:                       {results['Content_F1_Score']}\")\n",
    "#     print(f\"  Correct Words:                  {results['Correct_Words']}\")\n",
    "#     print(f\"  Missing Words:                  {results['Missing_Words']}\")\n",
    "#     print(f\"  Extra Words:                    {results['Extra_Words']}\")\n",
    "    \n",
    "#     print(\"\\nüî§ CHARACTER-LEVEL CONTENT METRICS\")\n",
    "#     print(\"-\" * 60)\n",
    "#     print(f\"  Precision:                      {results['Character_Content_Precision']}\")\n",
    "#     print(f\"  Recall:                         {results['Character_Content_Recall']}\")\n",
    "#     print(f\"  F1 Score:                       {results['Character_Content_F1']}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# # === USAGE EXAMPLE ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Specify your file paths\n",
    "#     ground_truth_file = \"original.txt\"\n",
    "#     extracted_text_file = \"predicted.txt\"\n",
    "    \n",
    "#     # Run evaluation\n",
    "#     results = evaluate_ocr_from_files(ground_truth_file, extracted_text_file)\n",
    "    \n",
    "#     # Display results\n",
    "#     print_results(results)\n",
    "    \n",
    "#     # Optionally save results to JSON\n",
    "#     # import json\n",
    "#     # with open('evaluation_results.json', 'w') as f:\n",
    "#     #     json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0741a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class MarkdownEvaluator:\n",
    "    def __init__(self, header_threshold=0.8):\n",
    "        self.header_threshold = header_threshold\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalizes text for comparison:\n",
    "        1. Removes markdown formatting (*, _, `) but KEEPS pipes | for tables.\n",
    "        2. Normalizes whitespace.\n",
    "        3. Lowers case (optional, strictness depends on use case).\n",
    "        \"\"\"\n",
    "        # Remove bold/italic/code markers\n",
    "        text = re.sub(r'[*_`]', '', text) \n",
    "        # Normalize whitespace (tabs/newlines -> single space)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def parse_markdown(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses text into sections. \n",
    "        If no headers (#) are found, treats the whole text as a 'Document' section.\n",
    "        \"\"\"\n",
    "        # Regex for standard Markdown headers (# Header)\n",
    "        pattern = re.compile(r'(^|\\n)(#+)\\s*(.*?)(?=\\n#|\\Z)', re.DOTALL)\n",
    "        sections = {}\n",
    "        \n",
    "        matches = list(pattern.finditer(text))\n",
    "        \n",
    "        # FAILSAFE: If no headers found, treat entire text as body content\n",
    "        if not matches:\n",
    "            sections['Whole Document'] = {\n",
    "                'title': 'Whole Document',\n",
    "                'content': text.strip()\n",
    "            }\n",
    "            return sections\n",
    "\n",
    "        # If headers exist, parse normally\n",
    "        if matches[0].start() > 0:\n",
    "            preamble = text[:matches[0].start()].strip()\n",
    "            if preamble:\n",
    "                sections['PREAMBLE'] = {'title': 'PREAMBLE', 'content': preamble}\n",
    "\n",
    "        for match in matches:\n",
    "            hashes, title, content = match.group(2), match.group(3), match.group(0)\n",
    "            # Remove the header line itself from the content to avoid duplication\n",
    "            content_only = content.replace(f\"{hashes} {title}\", \"\", 1).strip()\n",
    "            \n",
    "            sections[title.strip()] = {\n",
    "                'title': title.strip(),\n",
    "                'content': content_only\n",
    "            }\n",
    "            \n",
    "        return sections\n",
    "\n",
    "    def get_diff_highlight(self, a: str, b: str) -> str:\n",
    "        \"\"\"Helper to show exactly where characters differ.\"\"\"\n",
    "        s = SequenceMatcher(None, a, b)\n",
    "        diff_out = []\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            if tag == 'replace':\n",
    "                diff_out.append(f\"MISMATCH: '{a[i1:i2]}' vs '{b[j1:j2]}'\")\n",
    "            elif tag == 'delete':\n",
    "                diff_out.append(f\"MISSING in Pred: '{a[i1:i2]}'\")\n",
    "            elif tag == 'insert':\n",
    "                diff_out.append(f\"EXTRA in Pred: '{b[j1:j2]}'\")\n",
    "        return \" | \".join(diff_out)\n",
    "\n",
    "    def evaluate(self, gt_text: str, pred_text: str) -> dict:\n",
    "        gt_sections = self.parse_markdown(gt_text)\n",
    "        pred_sections = self.parse_markdown(pred_text)\n",
    "        \n",
    "        results = {'matches': [], 'score_sum': 0, 'count': 0}\n",
    "        \n",
    "        # Combine Title + Content for comparison to catch everything\n",
    "        def get_full_text(sec):\n",
    "            # If it's the \"Whole Document\" fallback, just return content\n",
    "            if sec['title'] == 'Whole Document':\n",
    "                return sec['content']\n",
    "            return f\"{sec['title']} {sec['content']}\"\n",
    "\n",
    "        matched_pred_keys = set()\n",
    "\n",
    "        for gt_key, gt_data in gt_sections.items():\n",
    "            best_match = None\n",
    "            best_score = 0.0\n",
    "            \n",
    "            gt_full_clean = self.clean_text(get_full_text(gt_data))\n",
    "            \n",
    "            # Find best matching section in Pred\n",
    "            for pred_key, pred_data in pred_sections.items():\n",
    "                if pred_key in matched_pred_keys: continue\n",
    "                \n",
    "                # Compare fuzzy headers, OR if we are in \"Whole Document\" mode, compare full text\n",
    "                if gt_key == \"Whole Document\" or pred_key == \"Whole Document\":\n",
    "                     # If parsing failed, we force a comparison of the body\n",
    "                    header_sim = 1.0 \n",
    "                else:\n",
    "                    header_sim = SequenceMatcher(None, gt_key, pred_key).ratio()\n",
    "\n",
    "                if header_sim > self.header_threshold:\n",
    "                    pred_full_clean = self.clean_text(get_full_text(pred_data))\n",
    "                    content_sim = SequenceMatcher(None, gt_full_clean, pred_full_clean).ratio()\n",
    "                    \n",
    "                    if content_sim > best_score:\n",
    "                        best_score = content_sim\n",
    "                        best_match = pred_key\n",
    "\n",
    "            if best_match:\n",
    "                matched_pred_keys.add(best_match)\n",
    "                \n",
    "                # Get raw texts for diffing\n",
    "                gt_raw = self.clean_text(get_full_text(gt_data))\n",
    "                pred_raw = self.clean_text(get_full_text(pred_sections[best_match]))\n",
    "                \n",
    "                print(f\"Cleaned ground truth: {gt_raw}\")\n",
    "                print(f\"Cleaned prediction  : {pred_raw}\")\n",
    "\n",
    "                diff_notes = \"\"\n",
    "                if best_score < 1.0:\n",
    "                    diff_notes = self.get_diff_highlight(gt_raw, pred_raw)\n",
    "\n",
    "                results['matches'].append({\n",
    "                    'section': gt_key,\n",
    "                    'score': best_score,\n",
    "                    'diff': diff_notes\n",
    "                })\n",
    "                results['score_sum'] += best_score\n",
    "                results['count'] += 1\n",
    "            else:\n",
    "                # Missing section penalty\n",
    "                results['matches'].append({'section': gt_key, 'score': 0.0, 'diff': \"Section Missing\"})\n",
    "                results['count'] += 1\n",
    "\n",
    "        results['final_score'] = results['score_sum'] / results['count'] if results['count'] > 0 else 0\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc37eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned ground truth: 03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂±‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫. 04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫. 05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É ‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±. ‡∂Ö) 1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í. 4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö. (‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
      "Cleaned prediction  : 03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂± ‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫. 04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫. 05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É ‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±. (‡∂Ö) 1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫. 3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í. 4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö. (‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
      "Overall Score: 0.9989\n",
      "----------------------------------------\n",
      "Section: Whole Document\n",
      "Score:   0.9989\n",
      "Errors:  EXTRA in Pred: ' ' | EXTRA in Pred: '('\n"
     ]
    }
   ],
   "source": [
    "gt = \"\"\n",
    "pred = \"\"\n",
    "with open(\"o.txt\", \"r\") as original_file:\n",
    "    for line in original_file.readlines():\n",
    "        gt = gt + \"\\n\" + line\n",
    "\n",
    "with open(\"p.txt\", \"r\") as pred_file:\n",
    "    for line in pred_file.readlines():\n",
    "        pred = pred + \"\\n\" + line\n",
    "\n",
    "evaluator = MarkdownEvaluator()\n",
    "metrics = evaluator.evaluate(gt, pred)\n",
    "\n",
    "print(f\"Overall Score: {metrics['final_score']:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "for m in metrics['matches']:\n",
    "    print(f\"Section: {m['section']}\")\n",
    "    print(f\"Score:   {m['score']:.4f}\")\n",
    "    if m['diff']:\n",
    "        print(f\"Errors:  {m['diff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1622cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.54.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (1.109.1)\n",
      "Collecting langfuse\n",
      "  Downloading langfuse-3.10.5-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting openinference-instrumentation-google-genai\n",
      "  Downloading openinference_instrumentation_google_genai-0.1.8-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (2.11.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from openai) (4.67.1)\n",
      "Collecting backoff>=1.10.0 (from langfuse)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from langfuse) (1.38.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from langfuse) (24.2)\n",
      "Collecting wrapt<2.0,>=1.14 (from langfuse)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.72.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in c:\\users\\eshan\\.conda\\envs\\eng\\lib\\site-packages (from opentelemetry-proto==1.39.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (6.32.1)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting openinference-instrumentation>=0.1.17 (from openinference-instrumentation-google-genai)\n",
      "  Downloading openinference_instrumentation-0.1.42-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting openinference-semantic-conventions (from openinference-instrumentation-google-genai)\n",
      "  Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-instrumentation (from openinference-instrumentation-google-genai)\n",
      "  Downloading opentelemetry_instrumentation-0.60b0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\eshan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading google_genai-1.54.0-py3-none-any.whl (262 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Downloading langfuse-3.10.5-py3-none-any.whl (398 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading openinference_instrumentation_google_genai-0.1.8-py3-none-any.whl (23 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading openinference_instrumentation-0.1.42-py3-none-any.whl (30 kB)\n",
      "Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl (10 kB)\n",
      "Downloading opentelemetry_instrumentation-0.60b0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: wrapt, websockets, opentelemetry-proto, openinference-semantic-conventions, backoff, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-instrumentation, google-genai, opentelemetry-exporter-otlp-proto-http, openinference-instrumentation, openinference-instrumentation-google-genai, langfuse\n",
      "\n",
      "  Attempting uninstall: wrapt\n",
      "\n",
      "    Found existing installation: wrapt 2.0.1\n",
      "\n",
      "    Uninstalling wrapt-2.0.1:\n",
      "\n",
      "      Successfully uninstalled wrapt-2.0.1\n",
      "\n",
      "   ----------------------------------------  0/15 [wrapt]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   -- -------------------------------------  1/15 [websockets]\n",
      "   ----- ----------------------------------  2/15 [opentelemetry-proto]\n",
      "   ----- ----------------------------------  2/15 [opentelemetry-proto]\n",
      "   ----- ----------------------------------  2/15 [opentelemetry-proto]\n",
      "   ---------- -----------------------------  4/15 [backoff]\n",
      "   ---------- -----------------------------  4/15 [backoff]\n",
      "   --------- -----------------  5/15 [opentelemetry-exporter-otlp-proto-common]\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "   --------- -----------------  5/15 [opentelemetry-exporter-otlp-proto-common]\n",
      "    Found existing installation: opentelemetry-api 1.38.0\n",
      "   --------- -----------------  5/15 [opentelemetry-exporter-otlp-proto-common]\n",
      "    Uninstalling opentelemetry-api-1.38.0:\n",
      "   --------- -----------------  5/15 [opentelemetry-exporter-otlp-proto-common]\n",
      "      Successfully uninstalled opentelemetry-api-1.38.0\n",
      "   --------- -----------------  5/15 [opentelemetry-exporter-otlp-proto-common]\n",
      "   ---------------- -----------------------  6/15 [opentelemetry-api]\n",
      "   ---------------- -----------------------  6/15 [opentelemetry-api]\n",
      "   ---------------- -----------------------  6/15 [opentelemetry-api]\n",
      "   ---------------- -----------------------  6/15 [opentelemetry-api]\n",
      "   ---------------- -----------------------  6/15 [opentelemetry-api]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------- -----------------  7/15 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   --------------------- ------------------  8/15 [opentelemetry-sdk]\n",
      "   ---------------------- ---------------  9/15 [opentelemetry-instrumentation]\n",
      "   ---------------------- ---------------  9/15 [opentelemetry-instrumentation]\n",
      "   ---------------------- ---------------  9/15 [opentelemetry-instrumentation]\n",
      "   ---------------------- ---------------  9/15 [opentelemetry-instrumentation]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   -------------------------- ------------- 10/15 [google-genai]\n",
      "   --------------------- ------- 11/15 [opentelemetry-exporter-otlp-proto-http]\n",
      "   --------------------- ------- 11/15 [opentelemetry-exporter-otlp-proto-http]\n",
      "   ------------------------------ ------- 12/15 [openinference-instrumentation]\n",
      "   ------------------------------ ------- 12/15 [openinference-instrumentation]\n",
      "   --------------------- --- 13/15 [openinference-instrumentation-google-genai]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ------------------------------------- -- 14/15 [langfuse]\n",
      "   ---------------------------------------- 15/15 [langfuse]\n",
      "\n",
      "Successfully installed backoff-2.2.1 google-genai-1.54.0 langfuse-3.10.5 openinference-instrumentation-0.1.42 openinference-instrumentation-google-genai-0.1.8 openinference-semantic-conventions-0.1.25 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-http-1.39.0 opentelemetry-instrumentation-0.60b0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 websockets-15.0.1 wrapt-1.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai openai langfuse openinference-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys ‚úã\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03939acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor\n",
    " \n",
    "GoogleGenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: 12.pdf...\n",
      "File uploaded successfully. URI: https://generativelanguage.googleapis.com/v1beta/files/8z67lmfjpidv\n",
      "\n",
      "Requesting transcription and translation...\n",
      "--- Transcription/Translation Result ---\n",
      "Please find the transcription and translation of the provided image below. The OCR output was highly inaccurate and contained multiple languages; therefore, a manual transcription from the image was performed.\n",
      "\n",
      "---\n",
      "\n",
      "## Page 1 Transcription and Translation\n",
      "\n",
      "*(Note: This page contains two recipes, one in the left column and one in the right column.)*\n",
      "\n",
      "**Left Column: ‡∂Ö‡∂π ‡∂¥‡∑î‡∂©‡∑í‡∂Ç (Amba Pu·∏çin) - Mango Pudding**\n",
      "\n",
      "**‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫: (Ava≈õya dravya:) - Required ingredients:**\n",
      "\n",
      "1.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1¬Ω ‡∂ö‡∑ä ‡∂ã‡∂´‡∑î‡∑Ä‡∂≠‡∑î‡∂ª‡∑ô‡∂±‡∑ä ‡∂≠‡∂∏‡∑ä‡∂∂‡∑è ‡∂¥‡∑ô‡∂ª‡∑è ‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (K≈çpi k≈çppha 1¬Ω k u·πáuvaturen thambƒÅ perƒÅ ganna.)\n",
      "    Boil 1¬Ω cups of coffee in hot water and strain.\n",
      "2.  ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∂ö‡∑Ñ ‡∂∏‡∂Ø 2‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∂∏ ‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (Biththara kaha mada 2k wenama ganna.)\n",
      "    Take 2 egg yolks separately.\n",
      "3.  ‡∂†‡∑ì‡∑É‡∑ä ‡∂ö‡∑ê‡∂∂‡∂Ω‡∑í 9‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´ ‡∑É‡∑í‡∑Ñ‡∑í‡∂±‡∑ä‡∑Ä ‡∂ö‡∂¥‡∑è‡∂ú‡∂±‡∑ä‡∂±.\n",
      "    (Chƒ´s k√§bali 9k pama·πáa sihinwa kapƒÅganna.)\n",
      "    Cut about 9 pieces of cheese thinly.\n",
      "    *(There's a small, faint drawing below this, with text that looks like \"‡∂≠‡∑ö‡∂ß ‡∑É‡∑ì‡∂±‡∑í ‡∂ö‡∑î‡∂©‡∑î\" (Tƒì·π≠a sƒ´ni ku·∏çu) - Sugar powder for tea)*\n",
      "\n",
      "---\n",
      "\n",
      "**Right Column: ‡∂ö‡∑ù‡∂¥‡∑í ‡∂¥‡∑î‡∂©‡∑í‡∂Ç (K≈çpi Pu·∏çin) - Coffee Pudding**\n",
      "\n",
      "**‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫: (Ava≈õya dravya:) - Required ingredients:**\n",
      "\n",
      "1.  ‡∑É‡∑ì‡∂±‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/4‡∂ö‡∑ä\n",
      "    (Sƒ´ni k≈çppha 1/4k)\n",
      "    1/4 cup of sugar\n",
      "2.  ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/4‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/4k)\n",
      "    1/4 cup of milk\n",
      "3.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑î‡∂©‡∑î ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 1/2‡∂ö‡∑ä\n",
      "    (K≈çpi ku·∏çu tƒì h√§ndi 1/2k)\n",
      "    1/2 teaspoon of coffee powder\n",
      "4.  ‡∂¢‡∑ô‡∂Ω‡∂ß‡∑í‡∂±‡∑ä ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Jela·π≠in tƒì h√§ndi 2k)\n",
      "    2 teaspoons of gelatin\n",
      "5.  ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∂ö‡∑Ñ ‡∂∏‡∂Ø 2‡∂ö‡∑ä\n",
      "    (Biththara kaha mada 2k)\n",
      "    2 egg yolks\n",
      "6.  ‡∑É‡∑ì‡∂±‡∑í ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 3‡∂ö‡∑ä\n",
      "    (Sƒ´ni mƒìsa h√§ndi 3k)\n",
      "    3 tablespoons of sugar\n",
      "7.  ‡∂∂‡∂ß‡∂ª‡∑ä ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Ba·π≠ar mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of butter\n",
      "8.  ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/2k)\n",
      "    1/2 cup of milk\n",
      "9.  ‡∂ö‡∑ù‡∂¥‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (K≈çpi k≈çppha 1/2k)\n",
      "    1/2 cup of coffee\n",
      "10. ‡∂∂‡∑í‡∑É‡∑ä‡∂ö‡∂ß‡∑ä ‡∂ö‡∑î‡∂©‡∑î ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Bis ka·π≠ ku·∏çu mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of biscuit crumbs\n",
      "11. ‡∑Ä‡∑ê‡∂±‡∑í‡∂Ω‡∑è ‡∂≠‡∑ö ‡∑Ñ‡∑ê‡∂≥‡∑í 1‡∂ö‡∑ä\n",
      "    (WanilƒÅ tƒì h√§ndi 1k)\n",
      "    1 teaspoon of vanilla\n",
      "12. ‡∂∂‡∑í‡∂≠‡∑ä‡∂≠‡∂ª ‡∑É‡∑î‡∂Ø‡∑î ‡∂∏‡∂Ø 2‡∂ö‡∑ä\n",
      "    (Biththara sudu mada 2k)\n",
      "    2 egg whites\n",
      "13. ‡∂Ö‡∂∫‡∑í‡∑É‡∑í‡∂Ç ‡∑É‡∑ì‡∂±‡∑í ‡∂∏‡∑ö‡∑É ‡∑Ñ‡∑ê‡∂≥‡∑í 2‡∂ö‡∑ä\n",
      "    (Aisin sƒ´ni mƒìsa h√§ndi 2k)\n",
      "    2 tablespoons of icing sugar\n",
      "14. ‡∂¢‡∑ô‡∂Ω‡∂ß‡∑í‡∂±‡∑ä ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Jela·π≠in k≈çppha 1/2k)\n",
      "    1/2 cup of gelatin\n",
      "15. ‡∂ö‡∑í‡∂ª‡∑í ‡∂ö‡∑ù‡∂¥‡∑ä‡∂¥ 1/2‡∂ö‡∑ä\n",
      "    (Kiri k≈çppha 1/2k)\n",
      "    1/2 cup of milk\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "*   ‡∂∏‡∑ö ‡∑É‡∑í‡∂∫‡∂Ω‡∑î ‡∂Ø‡∑ä‚Äç‡∂ª‡∑Ä‡∑ä‚Äç‡∂∫ ‡∑Ñ‡∑ú‡∂≥‡∑í‡∂±‡∑ä ‡∂∏‡∑í‡∑Å‡∑ä‚Äç‡∂ª ‡∂ö‡∂ª.\n",
      "    (Mƒì siyalu dravya ho≈àdin mishra kara.)\n",
      "    Mix all these ingredients well.\n",
      "*   ‡∂ß‡∑ä‚Äç‡∂ª‡∑ö ‡∂ë‡∂ö‡∂ö‡∂ß ‡∂Ø‡∂∏‡∑è.\n",
      "    (·π¨rƒì eka·π≠a damƒÅ.)\n",
      "    Put into a tray.\n",
      "*   ‡∂¥‡∑ê‡∂∫ 3-4‡∂ö‡∑ä ‡∑Å‡∑ì‡∂≠‡∂ö‡∂ª‡∂´‡∂∫‡∑ö ‡∂≠‡∂∂‡∂±‡∑ä‡∂±.\n",
      "    (Paya 3-4k ≈õƒ´thakara·πáayƒì thabanna.)\n",
      "    Leave in the refrigerator for 3-4 hours.\n",
      "*   ‡∂ö‡∑ë‡∂∏‡∂ß ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª (‡∂ú‡∂±‡∑ä‡∂±)\n",
      "    (K√§ma·π≠a genƒ´ma·π≠a perÃåa (ganna))\n",
      "    Before eating (serve).\n",
      "\n",
      "---\n",
      "\n",
      "## Page 2 Transcription and Translation\n",
      "\n",
      "*(This page contains general information, possibly related to an electricity service or organization.)*\n",
      "\n",
      "1.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∂∑‡∑è‡∂≠‡∑í‡∂ö‡∑è ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫\n",
      "    (Mƒ´rigama BhƒÅthikƒÅ Viduliya)\n",
      "    Mirigama Bhathika Electricity\n",
      "2.  ‡∑É‡∑ö‡∑Ä‡∂ö ‡∑É‡∂∏‡∑í‡∂≠‡∑í‡∂∫,\n",
      "    (Sƒìwaka Samithiya,)\n",
      "    Employees' Society,\n",
      "3.  ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è‡∂Ø‡∑ô‡∂± ‡∂≠‡∑ô‡∂Ω‡∑ä\n",
      "    (Viduliya labƒÅdena thel)\n",
      "    Oil that provides electricity\n",
      "4.  ‡∂∂‡∂Ω‡∂ú‡∂±‡∑ä‡∑Ä‡∑è ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (BalaganwƒÅ thibƒì.)\n",
      "    Has been empowered / is supplied.\n",
      "5.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∂∑‡∑è‡∂≠‡∑í‡∂ö‡∑è ‡∂¥‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∂ß\n",
      "    (Mƒ´rigama BhƒÅthikƒÅ pradƒì≈õaya·π≠a)\n",
      "    To the Mirigama Bhathika area\n",
      "6.  ‡∑Ä‡∑í‡∂Ø‡∑î‡∂Ω‡∑í‡∂∫ ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ì ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (Viduliya labƒÅ dƒ´ thibƒì.)\n",
      "    Electricity has been provided.\n",
      "7.  ‡∂∏‡∑ì‡∂ª‡∑í‡∂ú‡∂∏ ‡∑É‡∑ö‡∑Ä‡∂ö ‡∑É‡∂∏‡∑í‡∂≠‡∑í‡∂∫\n",
      "    (Mƒ´rigama Sƒìwaka Samithiya)\n",
      "    Mirigama Employees' Society\n",
      "8.  ‡∂¢‡∂∫‡∂ú‡∑ä‚Äç‡∂ª‡∑Ñ‡∂´‡∂∫\n",
      "    (Jayagraha·πáaya)\n",
      "    Victory\n",
      "9.  ‡∂Ω‡∂∂‡∑è ‡∂≠‡∑í‡∂∂‡∑ö.\n",
      "    (LabƒÅ thibƒì.)\n",
      "    Has achieved.\n",
      "--------------------------------------\n",
      "\n",
      "Deleting uploaded file: files/8z67lmfjpidv...\n",
      "File deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# --- Configuration ---\n",
    "IMAGE_PATH = \"12.pdf\"\n",
    "PROMPT = \"give me the transcription.\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Uploading file: {IMAGE_PATH}...\")\n",
    "try:\n",
    "    uploaded_file = client.files.upload(file=IMAGE_PATH)\n",
    "    print(f\"File uploaded successfully. URI: {uploaded_file.uri}\")\n",
    "\n",
    "    print(\"\\nRequesting transcription and translation...\")\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[PROMPT, uploaded_file]\n",
    "    )\n",
    "\n",
    "    print(\"--- Transcription/Translation Result ---\")\n",
    "    print(response.text)\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {IMAGE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'uploaded_file' in locals():\n",
    "        print(f\"\\nDeleting uploaded file: {uploaded_file.name}...\")\n",
    "        client.files.delete(name=uploaded_file.name)\n",
    "        print(\"File deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4037f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout analysis and transcription process started...\n",
      "  Uploading file: Generated Image December 09, 2025 - 4_41PM.jpeg...\n",
      "  ‚úì File uploaded: files/ra1uid8cd7xf (URI: https://generativelanguage.googleapis.com/v1beta/files/ra1uid8cd7xf)\n",
      "  Processing 1 file(s) with prompt...\n",
      "  Deleting uploaded file: files/ra1uid8cd7xf...\n",
      "  ‚úì File deleted: files/ra1uid8cd7xf\n",
      "Evaluation started...\n",
      "  Processing 0 file(s) with prompt...\n",
      "  An error occurred: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 44.043268586s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 44.043268586s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 384\u001b[0m\n\u001b[0;32m    381\u001b[0m   evaluate_with_gemini(prediction, ground_truth)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 384\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     langfuse_span_or_generation\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    452\u001b[0m         level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, status_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m is_return_type_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgenerator(result):\n",
      "Cell \u001b[1;32mIn[1], line 381\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    379\u001b[0m prediction, ground_truth \u001b[38;5;241m=\u001b[39m call_gemini(SYSTEM_INSTRUCTION_PROMPT, GROUND_TRUTH, model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Image December 09, 2025 - 4_41PM.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation started...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 381\u001b[0m \u001b[43mevaluate_with_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     langfuse_span_or_generation\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    452\u001b[0m         level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, status_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m is_return_type_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgenerator(result):\n",
      "Cell \u001b[1;32mIn[1], line 165\u001b[0m, in \u001b[0;36mevaluate_with_gemini\u001b[1;34m(prediction, ground_truth)\u001b[0m\n\u001b[0;32m    144\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124mYou are an evaluator. Compare the ground truth and the prediction.\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124mReturn a JSON object with exactly two fields:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# ---- CALL GEMINI ----\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.5-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# If call_gemini returns a tuple ‚Üí extract the text\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_output, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     langfuse_span_or_generation\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    452\u001b[0m         level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, status_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[0m, in \u001b[0;36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m is_return_type_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgenerator(result):\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mcall_gemini\u001b[1;34m(input, ground_truth, model_id, file_paths)\u001b[0m\n\u001b[0;32m     52\u001b[0m contents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m] \u001b[38;5;241m+\u001b[39m uploaded_files\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(uploaded_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file(s) with prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print(\"\\n--- Response ---\")\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(response.text)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# print(\"----------------\")\u001b[39;00m\n\u001b[0;32m     64\u001b[0m usage_meta \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage_metadata\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\models.py:5218\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5216\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5217\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5218\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5219\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[0;32m   5220\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5222\u001b[0m   function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n\u001b[0;32m   5223\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\models.py:4000\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   3997\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   3998\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 4000\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4001\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   4002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   4005\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4006\u001b[0m ):\n\u001b[0;32m   4007\u001b[0m   return_value \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse(sdk_http_response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1380\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1383\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SdkHttpResponse:\n\u001b[0;32m   1385\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m   1386\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m   1387\u001b[0m   )\n\u001b[1;32m-> 1388\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m   response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1390\u001b[0m       response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1391\u001b[0m   )\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1224\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[0;32m   1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[1;32m-> 1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    418\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m   1195\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   1196\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1199\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m   1200\u001b[0m   )\n\u001b[1;32m-> 1201\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m   1203\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m   1204\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m   response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mbody_segments[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Eshan\\.conda\\envs\\eng\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[0m, in \u001b[0;36mAPIError.raise_error\u001b[1;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[1;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 44.043268586s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '44s'}]}}"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse import get_client\n",
    "from langfuse import observe, propagate_attributes, Langfuse\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "\n",
    "@observe(name=\"call-gemini-common-fn\", as_type=\"generation\", capture_input=True, capture_output=True)\n",
    "def call_gemini(input, ground_truth, model_id=\"gemini-2.0-flash\", file_paths=None):\n",
    "    \"\"\"\n",
    "    Process multiple files with Gemini and trace with Langfuse.\n",
    "    \n",
    "    Args:\n",
    "        input: Text prompt/instruction\n",
    "        model_id: Gemini model to use\n",
    "        file_paths: List of file paths or single file path (string)\n",
    "    \"\"\"\n",
    "    with propagate_attributes(\n",
    "        user_id=\"eshanj\",\n",
    "        session_id=\"session_x\",\n",
    "        tags=[\"gemini\", \"eshan's-trace\", \"multi-file\"],\n",
    "        metadata={\"email\": \"eshan@fonixedu.com\"},\n",
    "        version=\"1.0.0\",\n",
    "    ):\n",
    "        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        \n",
    "        # Handle both single file and multiple files\n",
    "        if file_paths is None:\n",
    "            file_paths = []\n",
    "        elif isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        uploaded_files = []\n",
    "        \n",
    "        try:\n",
    "            # Upload all files\n",
    "            for file_path in file_paths:\n",
    "                print(f\"  Uploading file: {file_path}...\")\n",
    "                uploaded_file = client.files.upload(file=file_path)\n",
    "                uploaded_files.append(uploaded_file)\n",
    "                print(f\"  ‚úì File uploaded: {uploaded_file.name} (URI: {uploaded_file.uri})\")\n",
    "            \n",
    "            # Build content array: [prompt, file1, file2, file3, ...]\n",
    "            contents = [input] + uploaded_files\n",
    "            \n",
    "            print(f\"  Processing {len(uploaded_files)} file(s) with prompt...\")\n",
    "            response = client.models.generate_content(\n",
    "                model=model_id,\n",
    "                contents=contents,\n",
    "            )\n",
    "            \n",
    "            # print(\"\\n--- Response ---\")\n",
    "            # print(response.text)\n",
    "            # print(\"----------------\")\n",
    "            \n",
    "            usage_meta = response.usage_metadata\n",
    "    \n",
    "            # 1. Safely get values (handling None)\n",
    "            prompt_tokens = usage_meta.prompt_token_count or 0\n",
    "            candidate_tokens = usage_meta.candidates_token_count or 0\n",
    "            thought_tokens = usage_meta.thoughts_token_count or 0\n",
    "            cached_tokens = usage_meta.cached_content_token_count or 0\n",
    "            total_tokens = usage_meta.total_token_count or 0\n",
    "\n",
    "            # 2. Extract Image Tokens specifically (Useful for debugging)\n",
    "            # We loop through details to find IMAGE modality\n",
    "            image_tokens = 0\n",
    "            if usage_meta.prompt_tokens_details:\n",
    "                for detail in usage_meta.prompt_tokens_details:\n",
    "                    if detail.modality == \"IMAGE\":\n",
    "                        image_tokens += detail.token_count\n",
    "\n",
    "            # 3. Calculate \"Effective Output\" for Billing\n",
    "            # For Gemini, you pay for thoughts exactly like output text.\n",
    "            # So we must combine them for Langfuse's \"output\" field.\n",
    "            effective_output_tokens = candidate_tokens + thought_tokens\n",
    "\n",
    "            langfuse.update_current_trace(\n",
    "                input={\n",
    "                    \"prompt\": input,\n",
    "                    \"files\": [f.name for f in uploaded_files],\n",
    "                    \"file_count\": len(uploaded_files)\n",
    "                },\n",
    "                output=response.text,\n",
    "                metadata={\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            INPUT_PRICE_PER_TOKEN = 2 / 1000000  # Price per 1,000 tokens divided by 1,000\n",
    "            OUTPUT_PRICE_PER_TOKEN = 12 / 1000000\n",
    "            CACHING_PRICE_PER_TOKEN = 0.2 / 1000000\n",
    "\n",
    "            input_cost = prompt_tokens * INPUT_PRICE_PER_TOKEN\n",
    "            output_cost = effective_output_tokens * OUTPUT_PRICE_PER_TOKEN\n",
    "            cache_read_input_cost = cached_tokens * CACHING_PRICE_PER_TOKEN\n",
    "            total_cost = input_cost + output_cost + cache_read_input_cost\n",
    "            \n",
    "            langfuse.update_current_generation(\n",
    "                cost_details={\n",
    "                    # Here we assume the input and output cost are 1 USD each and half the price for cached tokens.\n",
    "                    \"input\": input_cost,\n",
    "                    \"cache_read_input_tokens\": cache_read_input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost, # if not set, it is derived from input + cache_read_input_tokens + output\n",
    "                },\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": effective_output_tokens, # Combined for correct cost\n",
    "                    \"cache_read_input_tokens\": cached_tokens \n",
    "                },\n",
    "            )\n",
    "\n",
    "            return response.text, ground_truth\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  ERROR: File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Clean up all uploaded files\n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    print(f\"  Deleting uploaded file: {uploaded_file.name}...\")\n",
    "                    client.files.delete(name=uploaded_file.name)\n",
    "                    print(f\"  ‚úì File deleted: {uploaded_file.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to delete {uploaded_file.name}: {e}\")\n",
    "\n",
    "import json\n",
    "\n",
    "@observe(as_type=\"evaluator\")\n",
    "def evaluate_with_gemini(prediction, ground_truth):\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an evaluator. Compare the ground truth and the prediction.\n",
    "    Return a JSON object with exactly two fields:\n",
    "\n",
    "    - \"score\": a float between 0 and 1 inclusive\n",
    "    - \"reason\": a short explanation of why the score was given\n",
    "\n",
    "    STRICT RULES:\n",
    "    - Output ONLY valid JSON.\n",
    "    - Do NOT include backticks, markdown, or any text outside the JSON.\n",
    "    - \"score\" MUST be a float.\n",
    "    - \"reason\" MUST be a string.\n",
    "\n",
    "    ground_truth:\n",
    "    {ground_truth}\n",
    "\n",
    "    prediction:\n",
    "    {prediction}\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- CALL GEMINI ----\n",
    "    raw_output = call_gemini(\n",
    "        eval_prompt,\n",
    "        ground_truth=None,\n",
    "        model_id=\"gemini-2.5-flash\",\n",
    "        file_paths=None\n",
    "    )\n",
    "\n",
    "    # If call_gemini returns a tuple ‚Üí extract the text\n",
    "    if isinstance(raw_output, tuple):\n",
    "        raw_output = raw_output[0]\n",
    "\n",
    "    print(\"Gemini Raw Output:\", raw_output)\n",
    "\n",
    "    clean_json = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(clean_json)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Gemini did not return valid JSON: {clean_json}\") from e\n",
    "\n",
    "    score = float(result[\"score\"])\n",
    "    reason = result[\"reason\"]\n",
    "\n",
    "    print(\" Score:\", score)\n",
    "    print(\" Reason:\", reason)\n",
    "\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"score\",\n",
    "        value=score,\n",
    "        comment=reason,\n",
    "    )\n",
    "\n",
    "    return score, reason\n",
    "\n",
    "@observe(name=\"gemini-qa-pipeline\", as_type=\"chain\")\n",
    "def main():\n",
    "  SYSTEM_INSTRUCTION_PROMPT = \"\"\"\n",
    "  You are an expert Document Layout and Diagram Analyzer. Your task is to process the provided handwritten document image, including any diagrams, tables, or complex layouts. Your entire response MUST be a single JSON object.\n",
    "\n",
    "  Task: Identify all meaningful blocks of content and extract the structural relationships between them.\n",
    "\n",
    "  JSON Schema: Output the prediction using the 'document_elements' array, where each object contains:\n",
    "  - id (string): A unique identifier (e.g., B1, N_Start).\n",
    "  - text (string): The transcribed content.\n",
    "  - type (enum): The element's function. Use only: TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR.\n",
    "  - bbox (array of 4 integers): Normalized coordinates [xmin, ymin, xmax, ymax]. All values MUST be integers between 0 and 100.\n",
    "  - relations (array of objects): A list of semantic connections.\n",
    "\n",
    "  Relations Schema (Inside relations):\n",
    "  - target_id (string): The id of the element it connects to.\n",
    "  - relation_type (enum): The connection type. Use: FLOWS_TO, IS_LABEL_FOR, VALUE_FOR.\n",
    "\n",
    "  Specific Instructions:\n",
    "  1. For diagrams, use DIAGRAM_NODE for shapes and DIAGRAM_ARROW for lines. Use FLOWS_TO to link the source node to the target node.\n",
    "  2. For forms/tables, use KEY_VALUE_PAIR. If a value is separated from its label, link them using VALUE_FOR.\n",
    "  \"\"\"\n",
    "\n",
    "  GROUND_TRUTH = \"\"\"```json\n",
    "  {\n",
    "    \"document_elements\": [\n",
    "      {\n",
    "        \"id\": \"T1\",\n",
    "        \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 2\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          9,\n",
    "          30,\n",
    "          12\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H1\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H1\",\n",
    "        \"text\": \"‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø ‡∂∏‡∑ñ‡∂ª‡∑ä‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (MRI - Magnetic Resonance Imaging Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          14,\n",
    "          92,\n",
    "          20\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P1\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P1\",\n",
    "        \"text\": \"‡∂ª‡∑ö‡∂©‡∑í‡∂∫‡∑ù ‡∂≠‡∂ª‡∂Ç‡∂ú ‡∑É‡∑Ñ ‡∂¥‡∑ä‚Äç‡∂ª‡∂∂‡∂Ω ‡∂†‡∑î‡∂∏‡∑ä‡∂∂‡∂ö ‡∂Ö‡∂±‡∑î‡∂±‡∑è‡∂Ø (‡∂Ø‡∑ô‡∑Å‡∑í‡∂ö) ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∂±‡∑ä‡∂≠‡∂ª ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä‡∑Ä‡∂Ω ‡∑É‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∂ª‡∑ñ‡∂¥ ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏ ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∂ª‡∑ù‡∂ú ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∂±‡∑í‡∂ª‡∑ä‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∂Ø ‡∂∏‡∑ô‡∂∏ ‡∂ª‡∑ñ‡∂¥ ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∑ö.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          23,\n",
    "          99,\n",
    "          36\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"T2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"T2\",\n",
    "        \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 3\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          39,\n",
    "          29,\n",
    "          42\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H2\",\n",
    "        \"text\": \"‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑î‡∂≠‡∑ä ‡∂≠‡∂±‡∑ä‡∂≠‡∑î ‡∂ª‡∑ö‡∂õ‡∑í‡∂∫ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (ECG - Electrocardiogram Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          44,\n",
    "          99,\n",
    "          50\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P2\",\n",
    "        \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫ ‡∂±‡∑í‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∫‡∑ú‡∂Ø‡∑è ‡∂ú‡∑ê‡∂±‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∑É‡∑í‡∂ß ‡∑Å‡∂ª‡∑ì‡∂ª‡∂∫‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö‡∑î‡∂≠‡∑ä ‡∂â‡∂±‡∑ä‡∂Ø‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∂∫‡∂±‡∑ä ‡∑Ä‡∑ô‡∂≠ ‡∂ª‡∑î‡∂∞‡∑í‡∂ª‡∂∫ ‡∑É‡∑ê‡∂¥‡∂∫‡∑ì‡∂∏‡∑ö ‡∂Ø‡∑ì ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂á‡∂≠‡∑í ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑î‡∂≠‡∑ä ‡∑É‡∑ä‡∂¥‡∂±‡∑ä‡∂Ø‡∂±‡∂∫‡∂ß ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂±‡∑í‡∂¥‡∂Ø‡∑Ä‡∑ô‡∂± ‡∂≠‡∂ª‡∂Ç‡∂ú ‡∂¥‡∑ä‚Äç‡∂ª‡∑É‡∑ä‡∂≠‡∑è‡∂ª‡∑í‡∂ö ‡∂ö‡∂©‡∂Ø‡∑è‡∑É‡∑í‡∂∫‡∂ö ‡∑É‡∂ß‡∑Ñ‡∂±‡∑ä ‡∑Ä‡∑ì‡∂∏ ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂Ø‡∑ì ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          53,\n",
    "          99,\n",
    "          67\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"T3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"T3\",\n",
    "        \"text\": \"‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ 4\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          69,\n",
    "          31,\n",
    "          72\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H3\",\n",
    "        \"text\": \"‡∑Ñ‡∑ò‡∂Ø ‡∂ª‡∑ù‡∂ú ‡∂±‡∑í‡∂ª‡∑ä ‡∂ú‡∂±‡∑ä‡∑Ä‡∑ì‡∂∏‡∑ö ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ (Cardiac Screening Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          74,\n",
    "          97,\n",
    "          80\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P3\",\n",
    "        \"text\": \"‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è‡∂ö‡∑è‡∂ª‡∑ì‡∂≠‡∑ä‡∑Ä‡∂∫ ‡∂¥‡∂ª‡∑í‡∂ú‡∂´‡∂ö ‡∂≠‡∑í‡∂ª‡∂∫‡∂ö ‡∂Ø‡∑ê‡∂ö‡∑ä‡∑Ä‡∑ì‡∂∏ ‡∂∏‡∑ô‡∂∏ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫ ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ö. ‡∑Ñ‡∑ò‡∂Ø‡∂∫‡∑ö ‡∂ª‡∑î‡∂∞‡∑í‡∂ª ‡∂±‡∑è‡∂Ω ‡∑É‡∑í‡∑Ñ‡∑í‡∂±‡∑ä ‡∑Ä‡∑ì‡∂∏ ‡∑Ä‡∑ê‡∂±‡∑í ‡∑Ä‡∑í‡∑Ä‡∑í‡∂∞ ‡∂Ü‡∑É‡∑è‡∂Ø‡∂± ‡∂≠‡∂≠‡∑ä‡∂≠‡∑ä‡∑Ä‡∂∫‡∂±‡∑ä ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∑Ä‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂ö‡∑è‡∂ª ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∫‡∑ú‡∂∏‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂∏‡∑ö ‡∂±‡∑í‡∑É‡∑è ‡∂¥‡∑Ñ‡∑É‡∑î ‡∑Ä‡∑ö.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          83,\n",
    "          98,\n",
    "          96\n",
    "        ],\n",
    "        \"relations\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Layout analysis and transcription process started...\")\n",
    "  prediction, ground_truth = call_gemini(SYSTEM_INSTRUCTION_PROMPT, GROUND_TRUTH, model_id=\"gemini-2.5-flash\", file_paths=\"Generated Image December 09, 2025 - 4_41PM.jpeg\")\n",
    "  print(\"Evaluation started...\")\n",
    "  evaluate_with_gemini(prediction, ground_truth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1abf81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
