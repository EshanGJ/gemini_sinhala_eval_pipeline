{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3becf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
      "Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl (94 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.5/1.5 MB 299.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 266.3 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 260.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 273.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 281.8 kB/s  0:00:05\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ---------------------------------------- 0/3 [rapidfuzz]\n",
      "   ------------- -------------------------- 1/3 [Levenshtein]\n",
      "   ---------------------------------------- 3/3 [python-Levenshtein]\n",
      "\n",
      "Successfully installed Levenshtein-0.27.3 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd64d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OCR EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä LAYOUT-AWARE METRICS (Order Matters)\n",
      "------------------------------------------------------------\n",
      "  CER (Character Error Rate):     0.2529\n",
      "  CER Accuracy:                   0.7471\n",
      "  WER (Word Error Rate):          0.2909\n",
      "  WER Accuracy:                   0.7091\n",
      "\n",
      "üìù CONTENT-ONLY METRICS (Order Ignored)\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.7707\n",
      "  Recall:                         0.7333\n",
      "  F1 Score:                       0.7516\n",
      "  Correct Words:                  121\n",
      "  Missing Words:                  44\n",
      "  Extra Words:                    36\n",
      "\n",
      "üî§ CHARACTER-LEVEL CONTENT METRICS\n",
      "------------------------------------------------------------\n",
      "  Precision:                      0.9725\n",
      "  Recall:                         0.9567\n",
      "  F1 Score:                       0.9645\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_layout_aware_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates Character Error Rate (CER) and Word Error Rate (WER) \n",
    "    using Levenshtein distance. These metrics are LAYOUT-AWARE \n",
    "    (order matters).\n",
    "    \"\"\"\n",
    "    gt_clean = ground_truth.strip()\n",
    "    ext_clean = extracted_text.strip()\n",
    "    \n",
    "    # Character Error Rate (CER)\n",
    "    char_distance = Levenshtein.distance(gt_clean, ext_clean)\n",
    "    char_length = len(gt_clean)\n",
    "    cer = char_distance / char_length if char_length > 0 else 0.0\n",
    "    \n",
    "    # Word Error Rate (WER)\n",
    "    gt_words = gt_clean.split()\n",
    "    ext_words = ext_clean.split()\n",
    "    word_distance = Levenshtein.distance(gt_words, ext_words)\n",
    "    word_length = len(gt_words)\n",
    "    wer = word_distance / word_length if word_length > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"Layout_Aware_CER\": round(cer, 4),\n",
    "        \"Layout_Aware_WER\": round(wer, 4),\n",
    "        \"Layout_Aware_CER_Accuracy\": round(1 - cer, 4),\n",
    "        \"Layout_Aware_WER_Accuracy\": round(1 - wer, 4),\n",
    "    }\n",
    "\n",
    "def calculate_content_only_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compares only text content with word frequency, completely \n",
    "    LAYOUT-AGNOSTIC (order doesn't matter). Best for evaluating \n",
    "    OCR text extraction quality independent of layout.\n",
    "    \"\"\"\n",
    "    gt_clean = ground_truth.strip().lower()\n",
    "    ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "    gt_counter = Counter(gt_clean.split())\n",
    "    ext_counter = Counter(ext_clean.split())\n",
    "    \n",
    "    # Words that appear with correct frequency\n",
    "    correct_counts = sum((gt_counter & ext_counter).values())\n",
    "    total_gt_words = sum(gt_counter.values())\n",
    "    total_ext_words = sum(ext_counter.values())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = correct_counts / total_gt_words if total_gt_words > 0 else 0.0\n",
    "    precision = correct_counts / total_ext_words if total_ext_words > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Additional insights\n",
    "    missing_words = total_gt_words - correct_counts\n",
    "    extra_words = total_ext_words - correct_counts\n",
    "    \n",
    "    return {\n",
    "        \"Content_Precision\": round(precision, 4),\n",
    "        \"Content_Recall\": round(recall, 4),\n",
    "        \"Content_F1_Score\": round(f1, 4),\n",
    "        \"Correct_Words\": correct_counts,\n",
    "        \"Missing_Words\": missing_words,\n",
    "        \"Extra_Words\": extra_words,\n",
    "        \"Total_GT_Words\": total_gt_words,\n",
    "        \"Total_Extracted_Words\": total_ext_words,\n",
    "    }\n",
    "\n",
    "def calculate_character_content_metrics(ground_truth: str, extracted_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Character-level content comparison (layout-agnostic).\n",
    "    Useful for languages with complex word boundaries like Sinhala/Tamil.\n",
    "    \"\"\"\n",
    "    gt_clean = ground_truth.strip().lower()\n",
    "    ext_clean = extracted_text.strip().lower()\n",
    "    \n",
    "    # Remove all whitespace for pure character comparison\n",
    "    gt_chars = Counter(gt_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "    ext_chars = Counter(ext_clean.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\"))\n",
    "    \n",
    "    correct_chars = sum((gt_chars & ext_chars).values())\n",
    "    total_gt_chars = sum(gt_chars.values())\n",
    "    total_ext_chars = sum(ext_chars.values())\n",
    "    \n",
    "    recall = correct_chars / total_gt_chars if total_gt_chars > 0 else 0.0\n",
    "    precision = correct_chars / total_ext_chars if total_ext_chars > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"Character_Content_Precision\": round(precision, 4),\n",
    "        \"Character_Content_Recall\": round(recall, 4),\n",
    "        \"Character_Content_F1\": round(f1, 4),\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_from_files(ground_truth_file: str, extracted_text_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Main function to evaluate OCR accuracy from two text files.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_file: Path to the ground truth text file\n",
    "        extracted_text_file: Path to the extracted/predicted text file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    try:\n",
    "        with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "            ground_truth = f.read()\n",
    "        with open(extracted_text_file, 'r', encoding='utf-8') as f:\n",
    "            extracted_text = f.read()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    results = {}\n",
    "    \n",
    "    # Layout-aware metrics (traditional CER/WER)\n",
    "    layout_metrics = calculate_layout_aware_metrics(ground_truth, extracted_text)\n",
    "    results.update(layout_metrics)\n",
    "    \n",
    "    # Content-only metrics (layout-agnostic)\n",
    "    content_metrics = calculate_content_only_metrics(ground_truth, extracted_text)\n",
    "    results.update(content_metrics)\n",
    "    \n",
    "    # Character-level content metrics\n",
    "    char_metrics = calculate_character_content_metrics(ground_truth, extracted_text)\n",
    "    results.update(char_metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Pretty print the evaluation results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìä LAYOUT-AWARE METRICS (Order Matters)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  CER (Character Error Rate):     {results['Layout_Aware_CER']}\")\n",
    "    print(f\"  CER Accuracy:                   {results['Layout_Aware_CER_Accuracy']}\")\n",
    "    print(f\"  WER (Word Error Rate):          {results['Layout_Aware_WER']}\")\n",
    "    print(f\"  WER Accuracy:                   {results['Layout_Aware_WER_Accuracy']}\")\n",
    "    \n",
    "    print(\"\\nüìù CONTENT-ONLY METRICS (Order Ignored)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Precision:                      {results['Content_Precision']}\")\n",
    "    print(f\"  Recall:                         {results['Content_Recall']}\")\n",
    "    print(f\"  F1 Score:                       {results['Content_F1_Score']}\")\n",
    "    print(f\"  Correct Words:                  {results['Correct_Words']}\")\n",
    "    print(f\"  Missing Words:                  {results['Missing_Words']}\")\n",
    "    print(f\"  Extra Words:                    {results['Extra_Words']}\")\n",
    "    \n",
    "    print(\"\\nüî§ CHARACTER-LEVEL CONTENT METRICS\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Precision:                      {results['Character_Content_Precision']}\")\n",
    "    print(f\"  Recall:                         {results['Character_Content_Recall']}\")\n",
    "    print(f\"  F1 Score:                       {results['Character_Content_F1']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# === USAGE EXAMPLE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your file paths\n",
    "    ground_truth_file = \"original.txt\"\n",
    "    extracted_text_file = \"predicted.txt\"\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_ocr_from_files(ground_truth_file, extracted_text_file)\n",
    "    \n",
    "    # Display results\n",
    "    print_results(results)\n",
    "    \n",
    "    # Optionally save results to JSON\n",
    "    # import json\n",
    "    # with open('evaluation_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0741a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score: 0.9989\n",
      "----------------------------------------\n",
      "Section: Whole Document\n",
      "Score:   0.9989\n",
      "Errors:  EXTRA in Pred: ' ' | EXTRA in Pred: '('\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class MarkdownEvaluator:\n",
    "    def __init__(self, header_threshold=0.8):\n",
    "        self.header_threshold = header_threshold\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalizes text for comparison:\n",
    "        1. Removes markdown formatting (*, _, `) but KEEPS pipes | for tables.\n",
    "        2. Normalizes whitespace.\n",
    "        3. Lowers case (optional, strictness depends on use case).\n",
    "        \"\"\"\n",
    "        # Remove bold/italic/code markers\n",
    "        text = re.sub(r'[*_`]', '', text) \n",
    "        # Normalize whitespace (tabs/newlines -> single space)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def parse_markdown(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses text into sections. \n",
    "        If no headers (#) are found, treats the whole text as a 'Document' section.\n",
    "        \"\"\"\n",
    "        # Regex for standard Markdown headers (# Header)\n",
    "        pattern = re.compile(r'(^|\\n)(#+)\\s*(.*?)(?=\\n#|\\Z)', re.DOTALL)\n",
    "        sections = {}\n",
    "        \n",
    "        matches = list(pattern.finditer(text))\n",
    "        \n",
    "        # FAILSAFE: If no headers found, treat entire text as body content\n",
    "        if not matches:\n",
    "            sections['Whole Document'] = {\n",
    "                'title': 'Whole Document',\n",
    "                'content': text.strip()\n",
    "            }\n",
    "            return sections\n",
    "\n",
    "        # If headers exist, parse normally\n",
    "        if matches[0].start() > 0:\n",
    "            preamble = text[:matches[0].start()].strip()\n",
    "            if preamble:\n",
    "                sections['PREAMBLE'] = {'title': 'PREAMBLE', 'content': preamble}\n",
    "\n",
    "        for match in matches:\n",
    "            hashes, title, content = match.group(2), match.group(3), match.group(0)\n",
    "            # Remove the header line itself from the content to avoid duplication\n",
    "            content_only = content.replace(f\"{hashes} {title}\", \"\", 1).strip()\n",
    "            \n",
    "            sections[title.strip()] = {\n",
    "                'title': title.strip(),\n",
    "                'content': content_only\n",
    "            }\n",
    "            \n",
    "        return sections\n",
    "\n",
    "    def get_diff_highlight(self, a: str, b: str) -> str:\n",
    "        \"\"\"Helper to show exactly where characters differ.\"\"\"\n",
    "        s = SequenceMatcher(None, a, b)\n",
    "        diff_out = []\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            if tag == 'replace':\n",
    "                diff_out.append(f\"MISMATCH: '{a[i1:i2]}' vs '{b[j1:j2]}'\")\n",
    "            elif tag == 'delete':\n",
    "                diff_out.append(f\"MISSING in Pred: '{a[i1:i2]}'\")\n",
    "            elif tag == 'insert':\n",
    "                diff_out.append(f\"EXTRA in Pred: '{b[j1:j2]}'\")\n",
    "        return \" | \".join(diff_out)\n",
    "\n",
    "    def evaluate(self, gt_text: str, pred_text: str) -> dict:\n",
    "        gt_sections = self.parse_markdown(gt_text)\n",
    "        pred_sections = self.parse_markdown(pred_text)\n",
    "        \n",
    "        results = {'matches': [], 'score_sum': 0, 'count': 0}\n",
    "        \n",
    "        # Combine Title + Content for comparison to catch everything\n",
    "        def get_full_text(sec):\n",
    "            # If it's the \"Whole Document\" fallback, just return content\n",
    "            if sec['title'] == 'Whole Document':\n",
    "                return sec['content']\n",
    "            return f\"{sec['title']} {sec['content']}\"\n",
    "\n",
    "        matched_pred_keys = set()\n",
    "\n",
    "        for gt_key, gt_data in gt_sections.items():\n",
    "            best_match = None\n",
    "            best_score = 0.0\n",
    "            \n",
    "            gt_full_clean = self.clean_text(get_full_text(gt_data))\n",
    "            \n",
    "            # Find best matching section in Pred\n",
    "            for pred_key, pred_data in pred_sections.items():\n",
    "                if pred_key in matched_pred_keys: continue\n",
    "                \n",
    "                # Compare fuzzy headers, OR if we are in \"Whole Document\" mode, compare full text\n",
    "                if gt_key == \"Whole Document\" or pred_key == \"Whole Document\":\n",
    "                     # If parsing failed, we force a comparison of the body\n",
    "                    header_sim = 1.0 \n",
    "                else:\n",
    "                    header_sim = SequenceMatcher(None, gt_key, pred_key).ratio()\n",
    "\n",
    "                if header_sim > self.header_threshold:\n",
    "                    pred_full_clean = self.clean_text(get_full_text(pred_data))\n",
    "                    content_sim = SequenceMatcher(None, gt_full_clean, pred_full_clean).ratio()\n",
    "                    \n",
    "                    if content_sim > best_score:\n",
    "                        best_score = content_sim\n",
    "                        best_match = pred_key\n",
    "\n",
    "            if best_match:\n",
    "                matched_pred_keys.add(best_match)\n",
    "                \n",
    "                # Get raw texts for diffing\n",
    "                gt_raw = self.clean_text(get_full_text(gt_data))\n",
    "                pred_raw = self.clean_text(get_full_text(pred_sections[best_match]))\n",
    "                \n",
    "                diff_notes = \"\"\n",
    "                if best_score < 1.0:\n",
    "                    diff_notes = self.get_diff_highlight(gt_raw, pred_raw)\n",
    "\n",
    "                results['matches'].append({\n",
    "                    'section': gt_key,\n",
    "                    'score': best_score,\n",
    "                    'diff': diff_notes\n",
    "                })\n",
    "                results['score_sum'] += best_score\n",
    "                results['count'] += 1\n",
    "            else:\n",
    "                # Missing section penalty\n",
    "                results['matches'].append({'section': gt_key, 'score': 0.0, 'diff': \"Section Missing\"})\n",
    "                results['count'] += 1\n",
    "\n",
    "        results['final_score'] = results['score_sum'] / results['count'] if results['count'] > 0 else 0\n",
    "        return results\n",
    "\n",
    "# ==========================================\n",
    "# TEST WITH YOUR SINHALA DATA\n",
    "# ==========================================\n",
    "\n",
    "gt = \"\"\"\n",
    "03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂±‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫.\n",
    "\n",
    "04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫.\n",
    "\n",
    "05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "\n",
    "06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "\n",
    "**‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É**\n",
    "\n",
    "‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±.\n",
    "\n",
    "‡∂Ö) 1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.                                             \n",
    "   2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.      \t\n",
    "   3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í.                           \t\n",
    "   4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö.                                                      \t\t\n",
    "\n",
    "(‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "\"\"\"\n",
    "\n",
    "pred = \"\"\"\n",
    "03. ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∑Ñ‡∑è ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂ú‡∂∏‡∂± ‡∂Ø ‡∂Ø‡∑í‡∂ú ‡∂ú‡∂∏‡∂±‡∂ö‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∂∏‡∑ô‡∂∏ \n",
    "‡∑Ä‡∂ª‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∑É‡∑è‡∂Ø‡∂∫ ‡∂ú‡∂∏‡∑ä ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂Ø‡∑ö‡∑Å‡∂∫‡∑ö ‡∑É‡∑ì‡∂∏‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ñ ‡∑Ä‡∑í‡∂ú‡∑É ‡∂∏ ‡∂á‡∂ª‡∂π‡∑ö. ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä ‡∂∏ \n",
    "‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂É‡∂ö‡∑ì‡∂ª‡∑ä ‡∂â‡∂ß‡∑î‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂Ø‡∑è‡∑Ö ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∑ö ‡∑Ä‡∑ö‡∂Ω‡∑è‡∑Ä ‡∂â‡∂ö‡∑ä‡∂∏‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂¥‡∑ô‡∂ª ‡∂Ö‡∂±‡∑ô‡∂ö‡∑ä \n",
    "‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∑Ñ‡∑è ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∂∫‡∑í‡∂∫‡∂≠‡∂∫ ‡∂≠‡∑ê‡∂∂‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î ‡∂∫.\n",
    "\n",
    "04. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑è ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä‡∑Ñ‡∑í ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑Ä‡∑í‡∂≠‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä \n",
    "‡∂±‡∑ú‡∂ö‡∂©‡∑Ä‡∑è ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂∫‡∑Ñ‡∂¥‡∂≠‡∑ä ‡∂∫.\n",
    "\n",
    "05. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß \n",
    "‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ô‡∂ö‡∂ö‡∂ß ‡∂ë‡∂ö‡∑ä ‡∂Ö‡∂Ø‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ø, ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö \n",
    "‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂±‡∑ä ‡∂ë‡∂ö‡∑í‡∂±‡∑ô‡∂ö‡∂ß ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä \n",
    "‡∂â‡∂ö‡∑è‡∂∏‡∂≠‡∑ä ‡∂Ø ‡∂ö‡∑í‡∑Ä ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "\n",
    "06. ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂±‡∑ä, ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ \n",
    "‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂â‡∂∏‡∑è‡∂∏‡∑ä‡∑Ä‡∂ª‡∂∫‡∑è ‡∂¥‡∑í‡∑Ö‡∑í‡∂¥‡∑ê‡∂Ø‡∑í‡∂∫ \n",
    "‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "\n",
    "**‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∑è‡∑É**\n",
    "\n",
    "‡∂¥‡∑Ñ‡∂≠ ‡∂¥‚Äç‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∑Ñ‡∂ª‡∑í ‡∂±‡∂∏‡∑ä ( ‚úî ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø, ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂±‡∂∏‡∑ä ( x ) ‡∂Ω‡∂ö‡∑î‡∂´ ‡∂Ø ‡∑Ä‡∂ª‡∑Ñ‡∂±‡∑ä ‡∂≠‡∑î‡∑Ö ‡∂∫‡∑ú‡∂Ø‡∂±‡∑ä‡∂±.\n",
    "\n",
    "(‡∂Ö)\n",
    "1. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ô‡∂ö‡∑î‡∂ß ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î ‡∂±‡∂∏‡∑ä ‡∂ö‡∂Ω‡∑è ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "2. ‡∑Ü‡∂¢‡∑ä‡∂ª‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∂ö‡∂ª ‡∑Ñ‡∑ù ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª ‡∂â‡∂ß‡∑î‡∂ö‡∑Ö ‡∂±‡∑ú‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂∫.\n",
    "3. ‚Äò‡∂¢‡∂∏‡∑ä‡∂ã ‡∂≠‡∂ö‡∑ä‡∂Ø‡∑ì‡∂∏‡∑ä‚Äô ‡∂∫‡∂±‡∑î ‡∂¥‡∑ô‡∂ª‡∂ß‡∑î ‡∂ö‡∂ª ‡∑É‡∂Ω‡∑è‡∂≠‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂∫‡∑í.\n",
    "4. ‡∂ú‡∂∏‡∂±‡∑ô‡∑Ñ‡∑í ‡∂∫‡∑ô‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∂±‡∑ä ‡∂ö‡∑í‡∑É‡∑í ‡∂∏ ‡∑É‡∑î‡∂±‡∑ä‡∂±‡∂≠‡∑ä ‡∑É‡∂Ω‡∑è‡∂≠‡∂∫‡∂ö‡∑ä ‡∂â‡∂ß‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö.\n",
    "\n",
    "(‡∂Ü) ‡∂î‡∂∂ ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∂ö‡∑É‡∑ä‡∂ª‡∑î ‡∂¢‡∂∏‡∑ä‡∂ã ‡∂ö‡∑Ö ‡∂Ö‡∂≠‡∑ä‡∂Ø‡∑ê‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂ö‡∑ô‡∂ß‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∑Ä‡∑í‡∂ú‚Äç‡∑ä‚Äç‡∂ª‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.\n",
    "\"\"\"\n",
    "\n",
    "evaluator = MarkdownEvaluator()\n",
    "metrics = evaluator.evaluate(gt, pred)\n",
    "\n",
    "print(f\"Overall Score: {metrics['final_score']:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "for m in metrics['matches']:\n",
    "    print(f\"Section: {m['section']}\")\n",
    "    print(f\"Score:   {m['score']:.4f}\")\n",
    "    if m['diff']:\n",
    "        print(f\"Errors:  {m['diff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37eabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
