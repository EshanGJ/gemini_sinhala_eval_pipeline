{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2379538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-genai openai langfuse openinference-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841e956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout analysis and transcription process started...\n",
      "  Uploading file: Generated Image December 09, 2025 - 4_41PM.jpeg...\n",
      "  ✓ File uploaded: files/3cj7whcdp4eo (URI: https://generativelanguage.googleapis.com/v1beta/files/3cj7whcdp4eo)\n",
      "  Processing 1 file(s) with prompt...\n",
      "  Deleting uploaded file: files/3cj7whcdp4eo...\n",
      "  ✓ File deleted: files/3cj7whcdp4eo\n",
      "Evaluation started...\n",
      "  Processing 0 file(s) with prompt...\n",
      "  An error occurred: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 45.408672434s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '45s'}]}}\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 45.408672434s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '45s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 425\u001b[39m\n\u001b[32m    422\u001b[39m   evaluate_with_gemini(prediction, ground_truth)\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     langfuse.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    451\u001b[39m     langfuse_span_or_generation.update(\n\u001b[32m    452\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m, status_message=\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 422\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    415\u001b[39m prediction, ground_truth = call_gemini(\n\u001b[32m    416\u001b[39m     INSTRUCTION_PROMPT, \n\u001b[32m    417\u001b[39m     GROUND_TRUTH, model_id=\u001b[33m\"\u001b[39m\u001b[33mgemini-flash-latest\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    418\u001b[39m     file_paths=\u001b[33m\"\u001b[39m\u001b[33mGenerated Image December 09, 2025 - 4_41PM.jpeg\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    419\u001b[39m     generation_config=generation_config\n\u001b[32m    420\u001b[39m     )\n\u001b[32m    421\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluation started...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m \u001b[43mevaluate_with_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    451\u001b[39m     langfuse_span_or_generation.update(\n\u001b[32m    452\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m, status_message=\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mevaluate_with_gemini\u001b[39m\u001b[34m(prediction, ground_truth)\u001b[39m\n\u001b[32m    145\u001b[39m eval_generation_config = types.GenerateContentConfig(\n\u001b[32m    146\u001b[39m     temperature=\u001b[32m0.0\u001b[39m,\n\u001b[32m    147\u001b[39m     top_p=\u001b[32m0.9\u001b[39m, \u001b[38;5;66;03m# Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Note: candidate_count is currently fixed at 1 for most models/use cases\u001b[39;00m\n\u001b[32m    162\u001b[39m )\n\u001b[32m    164\u001b[39m eval_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[33mReturn a JSON object with exactly two fields:\u001b[39m\n\u001b[32m    166\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m raw_output = \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_generation_config\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# If call_gemini returns a tuple → extract the text\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_output, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:455\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    451\u001b[39m     langfuse_span_or_generation.update(\n\u001b[32m    452\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m, status_message=\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mcall_gemini\u001b[39m\u001b[34m(input, ground_truth, model_id, file_paths, generation_config)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(uploaded_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file(s) with prompt...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     64\u001b[39m     response = client.models.generate_content(\n\u001b[32m     65\u001b[39m         model=model_id,\n\u001b[32m     66\u001b[39m         contents=contents,\n\u001b[32m     67\u001b[39m         config=generation_config,\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\models.py:5218\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5216\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5217\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5218\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5219\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5220\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5222\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5223\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\models.py:4000\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3997\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3998\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4005\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4006\u001b[39m ):\n\u001b[32m   4007\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1380\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1383\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1384\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1385\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m   response_body = (\n\u001b[32m   1390\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m   )\n\u001b[32m   1392\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1194\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m       method=http_request.method,\n\u001b[32m   1196\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 45.408672434s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '45s'}]}}"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse import get_client\n",
    "from langfuse import observe, propagate_attributes, Langfuse\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import json\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "\n",
    "@observe(name=\"call-gemini-common-fn\", as_type=\"generation\", capture_input=True, capture_output=True)\n",
    "def call_gemini(input, ground_truth, model_id=\"gemini-2.0-flash\", file_paths=None, generation_config=None):\n",
    "    \"\"\"\n",
    "    Process multiple files with Gemini and trace with Langfuse.\n",
    "    \n",
    "    Args:\n",
    "        input: Text prompt/instruction\n",
    "        model_id: Gemini model to use\n",
    "        file_paths: List of file paths or single file path (string)\n",
    "    \"\"\"\n",
    "    with propagate_attributes(\n",
    "        user_id=\"eshanj\",\n",
    "        session_id=\"session_x\",\n",
    "        tags=[\"gemini\", \"eshan's-trace\", \"multi-file\"],\n",
    "        metadata={\"email\": \"eshan@fonixedu.com\"},\n",
    "        version=\"1.0.0\",\n",
    "    ):\n",
    "        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        \n",
    "        # Handle both single file and multiple files\n",
    "        if file_paths is None:\n",
    "            file_paths = []\n",
    "        elif isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        uploaded_files = []\n",
    "        \n",
    "        try:\n",
    "            # Upload all files\n",
    "            for file_path in file_paths:\n",
    "                print(f\"  Uploading file: {file_path}...\")\n",
    "                uploaded_file = client.files.upload(file=file_path)\n",
    "                uploaded_files.append(uploaded_file)\n",
    "                print(f\"  ✓ File uploaded: {uploaded_file.name} (URI: {uploaded_file.uri})\")\n",
    "            \n",
    "            # Build content array: [prompt, file1, file2, file3, ...]\n",
    "            contents = [input] + uploaded_files\n",
    "            \n",
    "            print(f\"  Processing {len(uploaded_files)} file(s) with prompt...\")\n",
    "\n",
    "            if generation_config:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_id,\n",
    "                    contents=contents,\n",
    "                )\n",
    "            else:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_id,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "            \n",
    "            # print(\"\\n--- Response ---\")\n",
    "            # print(response.text)\n",
    "            # print(\"----------------\")\n",
    "            \n",
    "            usage_meta = response.usage_metadata\n",
    "    \n",
    "            prompt_tokens = usage_meta.prompt_token_count or 0\n",
    "            candidate_tokens = usage_meta.candidates_token_count or 0\n",
    "            thought_tokens = usage_meta.thoughts_token_count or 0\n",
    "            cached_tokens = usage_meta.cached_content_token_count or 0\n",
    "            total_tokens = usage_meta.total_token_count or 0\n",
    "\n",
    "            # loop through details to find IMAGE modality\n",
    "            image_tokens = 0\n",
    "            if usage_meta.prompt_tokens_details:\n",
    "                for detail in usage_meta.prompt_tokens_details:\n",
    "                    if detail.modality == \"IMAGE\":\n",
    "                        image_tokens += detail.token_count\n",
    "\n",
    "            effective_output_tokens = candidate_tokens + thought_tokens\n",
    "\n",
    "            langfuse.update_current_trace(\n",
    "                input={\n",
    "                    \"prompt\": input,\n",
    "                    \"files\": [f.name for f in uploaded_files],\n",
    "                    \"file_count\": len(uploaded_files)\n",
    "                },\n",
    "                output=response.text,\n",
    "                metadata={\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            INPUT_PRICE_PER_TOKEN = 0.3 / 1000000\n",
    "            OUTPUT_PRICE_PER_TOKEN = 2.5 / 1000000\n",
    "            CACHING_PRICE_PER_TOKEN = 0.03 / 1000000\n",
    "\n",
    "            input_cost = prompt_tokens * INPUT_PRICE_PER_TOKEN\n",
    "            output_cost = effective_output_tokens * OUTPUT_PRICE_PER_TOKEN\n",
    "            cache_read_input_cost = cached_tokens * CACHING_PRICE_PER_TOKEN\n",
    "            total_cost = input_cost + output_cost + cache_read_input_cost\n",
    "            \n",
    "            langfuse.update_current_generation(\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"cache_read_input_tokens\": cache_read_input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost,\n",
    "                },\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": effective_output_tokens,\n",
    "                    \"cache_read_input_tokens\": cached_tokens \n",
    "                },\n",
    "            )\n",
    "\n",
    "            return response.text, ground_truth\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  ERROR: File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    print(f\"  Deleting uploaded file: {uploaded_file.name}...\")\n",
    "                    client.files.delete(name=uploaded_file.name)\n",
    "                    print(f\"  ✓ File deleted: {uploaded_file.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to delete {uploaded_file.name}: {e}\")\n",
    "\n",
    "@observe(as_type=\"evaluator\")\n",
    "def evaluate_with_gemini(prediction, ground_truth):\n",
    "    eval_generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        top_p=0.9, # Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\n",
    "        top_k=40, # Number of top tokens to sample from (e.g., 40) OVERRIDES if temprature is 0\n",
    "        max_output_tokens=256, # Maximum tokens to generate\n",
    "        # frequency_penalty=0.1, # Penalizes tokens based on how often they have appeared (0.0 to 1.0)\n",
    "        # presence_penalty=0.1, # Penalizes tokens based on whether they have appeared at least once (0.0 to 1.0)\n",
    "        system_instruction=\"You are an evaluator. Compare the ground truth and the prediction.\",\n",
    "        # tools=tools_list,  # List of functions the model can call\n",
    "        response_mime_type=\"application/json\", # Forces output format (e.g., \"application/json\" for structured data)\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            # Set to a number of tokens to budget for internal thought process (0 disables)\n",
    "            thinking_budget=1024, \n",
    "            # Include the model's internal thoughts in the response (useful for debugging)\n",
    "            include_thoughts=True, \n",
    "        ),\n",
    "        # Note: candidate_count is currently fixed at 1 for most models/use cases\n",
    "    )\n",
    "\n",
    "    eval_prompt = f\"\"\"\n",
    "    Return a JSON object with exactly two fields:\n",
    "\n",
    "    - \"score\": a float between 0 and 1 inclusive\n",
    "    - \"reason\": a short explanation of why the score was given\n",
    "\n",
    "    STRICT RULES:\n",
    "    - Output ONLY valid JSON.\n",
    "    - Do NOT include backticks, markdown, or any text outside the JSON.\n",
    "    - \"score\" MUST be a float.\n",
    "    - \"reason\" MUST be a string.\n",
    "\n",
    "    ground_truth:\n",
    "    {ground_truth}\n",
    "\n",
    "    prediction:\n",
    "    {prediction}\n",
    "    \"\"\"\n",
    "\n",
    "    raw_output = call_gemini(\n",
    "        eval_prompt,\n",
    "        ground_truth=None,\n",
    "        model_id=\"gemini-2.5-flash\",\n",
    "        file_paths=None,\n",
    "        generation_config=eval_generation_config\n",
    "    )\n",
    "\n",
    "    # If call_gemini returns a tuple → extract the text\n",
    "    if isinstance(raw_output, tuple):\n",
    "        raw_output = raw_output[0]\n",
    "\n",
    "    print(\"Gemini Raw Output:\", raw_output)\n",
    "\n",
    "    clean_json = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(clean_json)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Gemini did not return valid JSON: {clean_json}\") from e\n",
    "\n",
    "    score = float(result[\"score\"])\n",
    "    reason = result[\"reason\"]\n",
    "\n",
    "    print(\" Score:\", score)\n",
    "    print(\" Reason:\", reason)\n",
    "\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"score\",\n",
    "        value=score,\n",
    "        comment=reason,\n",
    "    )\n",
    "\n",
    "    return score, reason\n",
    "\n",
    "@observe(name=\"gemini-qa-pipeline\", as_type=\"chain\")\n",
    "def main():\n",
    "  INSTRUCTION_PROMPT = \"\"\"\n",
    "  Task: Identify all meaningful blocks of content and extract the structural relationships between them.\n",
    "\n",
    "  JSON Schema: Output the prediction using the 'document_elements' array, where each object contains:\n",
    "  - id (string): A unique identifier (e.g., B1, N_Start).\n",
    "  - text (string): The transcribed content.\n",
    "  - type (enum): The element's function. Use only: TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR.\n",
    "  - bbox (array of 4 integers): Normalized coordinates [xmin, ymin, xmax, ymax]. All values MUST be integers between 0 and 100.\n",
    "  - relations (array of objects): A list of semantic connections.\n",
    "\n",
    "  Relations Schema (Inside relations):\n",
    "  - target_id (string): The id of the element it connects to.\n",
    "  - relation_type (enum): The connection type. Use: FLOWS_TO, IS_LABEL_FOR, VALUE_FOR.\n",
    "\n",
    "  Specific Instructions:\n",
    "  1. For diagrams, use DIAGRAM_NODE for shapes and DIAGRAM_ARROW for lines. Use FLOWS_TO to link the source node to the target node.\n",
    "  2. For forms/tables, use KEY_VALUE_PAIR. If a value is separated from its label, link them using VALUE_FOR.\n",
    "  \"\"\"\n",
    "\n",
    "  GROUND_TRUTH = \"\"\"```json\n",
    "  {\n",
    "    \"document_elements\": [\n",
    "      {\n",
    "        \"id\": \"T1\",\n",
    "        \"text\": \"උදාහරණ 2\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          9,\n",
    "          30,\n",
    "          12\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H1\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H1\",\n",
    "        \"text\": \"චුම්බක අනුනාද මූර්ණ යන්ත්‍රය (MRI - Magnetic Resonance Imaging Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          14,\n",
    "          92,\n",
    "          20\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P1\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P1\",\n",
    "        \"text\": \"රේඩියෝ තරංග සහ ප්‍රබල චුම්බක අනුනාද (දෙශික) මගින් ශරීරයේ අභ්‍යන්තර කොටස්වල සවිස්තරාත්මක රූප සටහන් ලබා ගැනීම මෙම යන්ත්‍රය මගින් සිදු වේ. රෝග හඳුනා ගැනීමේ දී මෙන් ම ප්‍රතිකාර නිර්ණය කිරීමේ දී ද මෙම රූප උපකාරී වේ.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          23,\n",
    "          99,\n",
    "          36\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"T2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"T2\",\n",
    "        \"text\": \"උදාහරණ 3\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          39,\n",
    "          29,\n",
    "          42\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H2\",\n",
    "        \"text\": \"විද්‍යුත් තන්තු රේඛිය යන්ත්‍රය (ECG - Electrocardiogram Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          44,\n",
    "          99,\n",
    "          50\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P2\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P2\",\n",
    "        \"text\": \"හෘද ස්පන්දනය නිරීක්ෂණය කිරීම සඳහා මෙම යන්ත්‍රය යොදා ගැනේ. හෘදයේ සිට ශරීරයේ අනෙකුත් ඉන්ද්‍රියයන් වෙත රුධිරය සැපයීමේ දී හෘදයේ ඇති වන විද්‍යුත් ස්පන්දනයට අනුව නිපදවෙන තරංග ප්‍රස්තාරික කඩදාසියක සටහන් වීම මෙහි දී සිදු වේ.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          53,\n",
    "          99,\n",
    "          67\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"T3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"T3\",\n",
    "        \"text\": \"උදාහරණ 4\",\n",
    "        \"type\": \"TITLE\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          69,\n",
    "          31,\n",
    "          72\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"H3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"H3\",\n",
    "        \"text\": \"හෘද රෝග නිර් ගන්වීමේ යන්ත්‍රය (Cardiac Screening Machine)\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          74,\n",
    "          97,\n",
    "          80\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"target_id\": \"P3\",\n",
    "            \"relation_type\": \"FLOWS_TO\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"P3\",\n",
    "        \"text\": \"හෘදයේ ක්‍රියාකාරීත්වය පරිගණක තිරයක දැක්වීම මෙම යන්ත්‍රය මගින් සිදු වේ. හෘදයේ රුධිර නාල සිහින් වීම වැනි විවිධ ආසාදන තත්ත්වයන් හඳුනා ගැනීමට හැකි වීමෙන් අවශ්‍ය ප්‍රතිකාර සඳහා යොමු කිරීමට මේ නිසා පහසු වේ.\",\n",
    "        \"type\": \"PARAGRAPH\",\n",
    "        \"bbox\": [\n",
    "          12,\n",
    "          83,\n",
    "          98,\n",
    "          96\n",
    "        ],\n",
    "        \"relations\": []\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        top_p=0.9, # Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\n",
    "        top_k=40, # Number of top tokens to sample from (e.g., 40) OVERRIDES if temprature is 0\n",
    "        max_output_tokens=8192, # Maximum tokens to generate\n",
    "        # frequency_penalty=0.1, # Penalizes tokens based on how often they have appeared (0.0 to 1.0)\n",
    "        # presence_penalty=0.1, # Penalizes tokens based on whether they have appeared at least once (0.0 to 1.0)\n",
    "        system_instruction=\"You are an expert Document Layout and Diagram Analyzer. Your task is to process the provided handwritten document image, including any diagrams, tables, or complex layouts. Your entire response MUST be a single JSON object.\",\n",
    "        # tools=tools_list,  # List of functions the model can call\n",
    "        response_mime_type=\"application/json\", # Forces output format (e.g., \"application/json\" for structured data)\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            # Set to a number of tokens to budget for internal thought process (0 disables)\n",
    "            thinking_budget=-1, \n",
    "            # Include the model's internal thoughts in the response (useful for debugging)\n",
    "            include_thoughts=True, \n",
    "        ),\n",
    "        # Note: candidate_count is currently fixed at 1 for most models/use cases\n",
    "    )\n",
    "\n",
    "  print(\"Layout analysis and transcription process started...\")\n",
    "  prediction, ground_truth = call_gemini(\n",
    "      INSTRUCTION_PROMPT, \n",
    "      GROUND_TRUTH, model_id=\"gemini-flash-latest\", \n",
    "      file_paths=\"Generated Image December 09, 2025 - 4_41PM.jpeg\",\n",
    "      generation_config=generation_config\n",
    "      )\n",
    "  print(\"Evaluation started...\")\n",
    "  evaluate_with_gemini(prediction, ground_truth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
