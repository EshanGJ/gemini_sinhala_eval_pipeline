{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2379538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-genai openai langfuse openinference-instrumentation-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout analysis and transcription process started...\n",
      "  Uploading file: Generated Image December 09, 2025 - 4_41PM.jpeg...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 475\u001b[39m\n\u001b[32m    471\u001b[39m   \u001b[38;5;66;03m# print(\"Evaluation started...\")\u001b[39;00m\n\u001b[32m    472\u001b[39m   \u001b[38;5;66;03m# evaluate_with_gemini(prediction, ground_truth)\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     langfuse.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 465\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    445\u001b[39m generation_config = types.GenerateContentConfig(\n\u001b[32m    446\u001b[39m       temperature=\u001b[32m0.0\u001b[39m,\n\u001b[32m    447\u001b[39m       top_p=\u001b[32m0.9\u001b[39m, \u001b[38;5;66;03m# Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    461\u001b[39m       \u001b[38;5;66;03m# Note: candidate_count is currently fixed at 1 for most models/use cases\u001b[39;00m\n\u001b[32m    462\u001b[39m   )\n\u001b[32m    464\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLayout analysis and transcription process started...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m prediction, ground_truth = \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mINSTRUCTION_PROMPT_SIMPLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGROUND_TRUTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-3-pro-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGenerated Image December 09, 2025 - 4_41PM.jpeg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\langfuse\\_client\\observe.py:412\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcall_gemini\u001b[39m\u001b[34m(input, ground_truth, model_id, file_paths, generation_config)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Uploading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     uploaded_file = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     uploaded_files.append(uploaded_file)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ File uploaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muploaded_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (URI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muploaded_file.uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\files.py:472\u001b[39m, in \u001b[36mFiles.upload\u001b[39m\u001b[34m(self, file, config)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    471\u001b[39m   fs_path = os.fspath(file)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m   return_file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m types.File._from_response(\n\u001b[32m    477\u001b[39m     response=return_file.json[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    478\u001b[39m     kwargs=config_model.model_dump() \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[32m    479\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1501\u001b[39m, in \u001b[36mBaseApiClient.upload_file\u001b[39m\u001b[34m(self, file_path, upload_url, upload_size, http_options)\u001b[39m\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_fd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\google\\genai\\_api_client.py:1558\u001b[39m, in \u001b[36mBaseApiClient._upload_fd\u001b[39m\u001b[34m(self, file, upload_url, upload_size, http_options)\u001b[39m\n\u001b[32m   1556\u001b[39m retry_count = \u001b[32m0\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count < MAX_RETRY_COUNT:\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_in_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1565\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m response.headers.get(\u001b[33m'\u001b[39m\u001b[33mx-goog-upload-status\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1566\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\eng\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse import get_client\n",
    "from langfuse import observe, propagate_attributes, Langfuse\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import json\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\")\n",
    ")\n",
    "\n",
    "@observe(name=\"call-gemini-common-fn\", as_type=\"generation\", capture_input=True, capture_output=True)\n",
    "def call_gemini(input, ground_truth, model_id=\"gemini-2.0-flash\", file_paths=None, generation_config=None):\n",
    "    \"\"\"\n",
    "    Process multiple files with Gemini and trace with Langfuse.\n",
    "    \n",
    "    Args:\n",
    "        input: Text prompt/instruction\n",
    "        model_id: Gemini model to use\n",
    "        file_paths: List of file paths or single file path (string)\n",
    "    \"\"\"\n",
    "    with propagate_attributes(\n",
    "        user_id=\"eshanj\",\n",
    "        session_id=\"session_x\",\n",
    "        tags=[\"gemini\", \"eshan's-trace\", \"multi-file\"],\n",
    "        metadata={\"email\": \"eshan@fonixedu.com\"},\n",
    "        version=\"1.0.0\",\n",
    "    ):\n",
    "        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        \n",
    "        # Handle both single file and multiple files\n",
    "        if file_paths is None:\n",
    "            file_paths = []\n",
    "        elif isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        uploaded_files = []\n",
    "        \n",
    "        try:\n",
    "            # Upload all files\n",
    "            for file_path in file_paths:\n",
    "                print(f\"  Uploading file: {file_path}...\")\n",
    "                uploaded_file = client.files.upload(file=file_path)\n",
    "                uploaded_files.append(uploaded_file)\n",
    "                print(f\"  ✓ File uploaded: {uploaded_file.name} (URI: {uploaded_file.uri})\")\n",
    "            \n",
    "            # Build content array: [prompt, file1, file2, file3, ...]\n",
    "            contents = [input] + uploaded_files\n",
    "            \n",
    "            print(f\"  Processing {len(uploaded_files)} file(s) with prompt...\")\n",
    "\n",
    "            if generation_config:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_id,\n",
    "                    contents=contents,\n",
    "                )\n",
    "            else:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_id,\n",
    "                    contents=contents,\n",
    "                    config=generation_config,\n",
    "                )\n",
    "            \n",
    "            # print(\"\\n--- Response ---\")\n",
    "            # print(response.text)\n",
    "            # print(\"----------------\")\n",
    "            \n",
    "            usage_meta = response.usage_metadata\n",
    "    \n",
    "            prompt_tokens = usage_meta.prompt_token_count or 0\n",
    "            candidate_tokens = usage_meta.candidates_token_count or 0\n",
    "            thought_tokens = usage_meta.thoughts_token_count or 0\n",
    "            cached_tokens = usage_meta.cached_content_token_count or 0\n",
    "            total_tokens = usage_meta.total_token_count or 0\n",
    "\n",
    "            # loop through details to find IMAGE modality\n",
    "            image_tokens = 0\n",
    "            if usage_meta.prompt_tokens_details:\n",
    "                for detail in usage_meta.prompt_tokens_details:\n",
    "                    if detail.modality == \"IMAGE\":\n",
    "                        image_tokens += detail.token_count\n",
    "\n",
    "            effective_output_tokens = candidate_tokens + thought_tokens\n",
    "\n",
    "            langfuse.update_current_trace(\n",
    "                input={\n",
    "                    \"prompt\": input,\n",
    "                    \"files\": [f.name for f in uploaded_files],\n",
    "                    \"file_count\": len(uploaded_files)\n",
    "                },\n",
    "                output=response.text,\n",
    "                metadata={\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            INPUT_PRICE_PER_TOKEN = 0.3 / 1000000\n",
    "            OUTPUT_PRICE_PER_TOKEN = 2.5 / 1000000\n",
    "            CACHING_PRICE_PER_TOKEN = 0.03 / 1000000\n",
    "\n",
    "            input_cost = prompt_tokens * INPUT_PRICE_PER_TOKEN\n",
    "            output_cost = effective_output_tokens * OUTPUT_PRICE_PER_TOKEN\n",
    "            cache_read_input_cost = cached_tokens * CACHING_PRICE_PER_TOKEN\n",
    "            total_cost = input_cost + output_cost + cache_read_input_cost\n",
    "            \n",
    "            langfuse.update_current_generation(\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"cache_read_input_tokens\": cache_read_input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost,\n",
    "                },\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": effective_output_tokens,\n",
    "                    \"cache_read_input_tokens\": cached_tokens \n",
    "                },\n",
    "            )\n",
    "\n",
    "            return response.text, ground_truth\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  ERROR: File not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    print(f\"  Deleting uploaded file: {uploaded_file.name}...\")\n",
    "                    client.files.delete(name=uploaded_file.name)\n",
    "                    print(f\"  ✓ File deleted: {uploaded_file.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to delete {uploaded_file.name}: {e}\")\n",
    "\n",
    "@observe(as_type=\"evaluator\")\n",
    "def evaluate_with_gemini(prediction, ground_truth):\n",
    "    eval_generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        top_p=0.9, # Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\n",
    "        top_k=40, # Number of top tokens to sample from (e.g., 40) OVERRIDES if temprature is 0\n",
    "        max_output_tokens=256, # Maximum tokens to generate\n",
    "        # frequency_penalty=0.1, # Penalizes tokens based on how often they have appeared (0.0 to 1.0)\n",
    "        # presence_penalty=0.1, # Penalizes tokens based on whether they have appeared at least once (0.0 to 1.0)\n",
    "        system_instruction=\"You are an evaluator. Compare the ground truth and the prediction.\",\n",
    "        # tools=tools_list,  # List of functions the model can call\n",
    "        response_mime_type=\"application/json\", # Forces output format (e.g., \"application/json\" for structured data)\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            # Set to a number of tokens to budget for internal thought process (0 disables)\n",
    "            thinking_budget=1024, \n",
    "            # Include the model's internal thoughts in the response (useful for debugging)\n",
    "            include_thoughts=True, \n",
    "        ),\n",
    "        # Note: candidate_count is currently fixed at 1 for most models/use cases\n",
    "    )\n",
    "\n",
    "    eval_prompt = f\"\"\"\n",
    "    Return a JSON object with exactly two fields:\n",
    "\n",
    "    - \"score\": a float between 0 and 1 inclusive\n",
    "    - \"reason\": a short explanation of why the score was given\n",
    "\n",
    "    STRICT RULES:\n",
    "    - Output ONLY valid JSON.\n",
    "    - Do NOT include backticks, markdown, or any text outside the JSON.\n",
    "    - \"score\" MUST be a float.\n",
    "    - \"reason\" MUST be a string.\n",
    "\n",
    "    ground_truth:\n",
    "    {ground_truth}\n",
    "\n",
    "    prediction:\n",
    "    {prediction}\n",
    "    \"\"\"\n",
    "\n",
    "    raw_output = call_gemini(\n",
    "        eval_prompt,\n",
    "        ground_truth=None,\n",
    "        model_id=\"gemini-3-pro-preview\",\n",
    "        file_paths=None,\n",
    "        generation_config=eval_generation_config\n",
    "    )\n",
    "\n",
    "    # If call_gemini returns a tuple → extract the text\n",
    "    if isinstance(raw_output, tuple):\n",
    "        raw_output = raw_output[0]\n",
    "\n",
    "    print(\"Gemini Raw Output:\", raw_output)\n",
    "\n",
    "    clean_json = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(clean_json)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Gemini did not return valid JSON: {clean_json}\") from e\n",
    "\n",
    "    score = float(result[\"score\"])\n",
    "    reason = result[\"reason\"]\n",
    "\n",
    "    print(\" Score:\", score)\n",
    "    print(\" Reason:\", reason)\n",
    "\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"score\",\n",
    "        value=score,\n",
    "        comment=reason,\n",
    "    )\n",
    "\n",
    "    return score, reason\n",
    "\n",
    "@observe(name=\"gemini-qa-pipeline\", as_type=\"chain\")\n",
    "def main():\n",
    "  INSTRUCTION_PROMPT_COMPLEX = \"\"\"\n",
    "  Task: Identify all meaningful blocks of content and extract the structural relationships between them.\n",
    "\n",
    "  JSON Schema: Output the prediction using the 'document_elements' array, where each object contains:\n",
    "  - id (string): A unique identifier (e.g., B1, N_Start).\n",
    "  - text (string): The transcribed content.\n",
    "  - type (enum): The element's function. Use only: TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR.\n",
    "  - bbox (array of 4 integers): Normalized coordinates [xmin, ymin, xmax, ymax]. All values MUST be integers between 0 and 100.\n",
    "  - relations (array of objects): A list of semantic connections.\n",
    "\n",
    "  Relations Schema (Inside relations):\n",
    "  - target_id (string): The id of the element it connects to.\n",
    "  - relation_type (enum): The connection type. Use: FLOWS_TO, IS_LABEL_FOR, VALUE_FOR.\n",
    "\n",
    "  Specific Instructions:\n",
    "  1. For diagrams, use DIAGRAM_NODE for shapes and DIAGRAM_ARROW for lines. Use FLOWS_TO to link the source node to the target node.\n",
    "  2. For forms/tables, use KEY_VALUE_PAIR. If a value is separated from its label, link them using VALUE_FOR.\n",
    "  \"\"\"\n",
    "\n",
    "  INSTRUCTION_PROMPT_SIMPLE = \"\"\"\n",
    "  Task: Identify all meaningful blocks of content and extract the structural relationships between them.\n",
    "\n",
    "  JSON Schema: Output using the 'document_elements' array, where each object contains:\n",
    "  - id (string): Unique identifier (e.g., B1, N1)\n",
    "  - text (string): The transcribed content\n",
    "  - type (enum): TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, DIAGRAM_ARROW, KEY_VALUE_PAIR\n",
    "  - relations (array): Semantic connections with:\n",
    "  - target_id (string): Connected element's id\n",
    "  - relation_type (enum): FLOWS_TO, IS_LABEL_FOR, VALUE_FOR\n",
    "  \n",
    "  Specific Instructions:\n",
    "  1. For diagrams: Use DIAGRAM_NODE for shapes, DIAGRAM_ARROW for lines. Link with FLOWS_TO.\n",
    "  2. For forms/tables: Use KEY_VALUE_PAIR. Link labels to values using VALUE_FOR.\n",
    "  \"\"\"\n",
    "\n",
    "  INSTRUCTION_PROMPT_MARKDOWN = \"\"\"\n",
    "  # Document Analysis\n",
    "\n",
    "  ## Task\n",
    "  Analyze the document and provide:\n",
    "  1. A markdown description of the page content (diagrams, tables, images, etc.)\n",
    "  2. Complete transcription of all text\n",
    "\n",
    "  ## Output Format\n",
    "\n",
    "  ### 1. Page Description\n",
    "  Provide a brief markdown summary describing:\n",
    "  - Document type and layout\n",
    "  - Visual elements present (diagrams, tables, charts, images)\n",
    "  - Overall structure\n",
    "\n",
    "  ### 2. Transcription\n",
    "  Output using the `document_elements` array:\n",
    "\n",
    "  | Field | Type | Description |\n",
    "  |-------|------|-------------|\n",
    "  | `id` | string | Unique identifier (e.g., B1, N1) |\n",
    "  | `text` | string | The transcribed content |\n",
    "  | `type` | enum | TITLE, PARAGRAPH, LIST, TABLE_CELL, DIAGRAM_NODE, KEY_VALUE_PAIR |\n",
    "  | `relations` | array | Connections: `target_id` and `relation_type` (FLOWS_TO, VALUE_FOR) |\n",
    "\n",
    "  ## Instructions\n",
    "  - **Text:** Transcribe all visible text\n",
    "  - **Diagrams:** Describe flow and connections using FLOWS_TO\n",
    "  - **Tables/Forms:** Link labels to values using VALUE_FOR\n",
    "  \"\"\"\n",
    "\n",
    "  INSTRUCTION_SIMPLE_PROMPT_MARKDOWN = \"\"\"\n",
    "  ## Task\n",
    "  Analyze the document image and provide:\n",
    "  1. **Transcribe** all text exactly as written\n",
    "  2. **Describe** any diagrams, tables, or visual elements\n",
    "\n",
    "  ## Output\n",
    "  Respond in **Markdown format**:\n",
    "  - Use headings, lists, and tables to match the document structure\n",
    "  - For diagrams, describe the flow (e.g., A → B → C)\n",
    "  - Mark unclear text as `[unclear]`\n",
    "  \"\"\"\n",
    "  \n",
    "  GROUND_TRUTH = ''\n",
    "\n",
    "  generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        top_p=0.9, # Nucleus sampling threshold (0.0 to 1.0) OVERRIDES if temprature is 0\n",
    "        top_k=40, # Number of top tokens to sample from (e.g., 40) OVERRIDES if temprature is 0\n",
    "        max_output_tokens=8192, # Maximum tokens to generate\n",
    "        # frequency_penalty=0.1, # Penalizes tokens based on how often they have appeared (0.0 to 1.0)\n",
    "        # presence_penalty=0.1, # Penalizes tokens based on whether they have appeared at least once (0.0 to 1.0)\n",
    "        system_instruction=\"You are an expert Document Analyzer.\",\n",
    "        # tools=tools_list,  # List of functions the model can call\n",
    "        # response_mime_type=\"application/json\", # Forces output format (e.g., \"application/json\" for structured data)\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            # Set to a number of tokens to budget for internal thought process (0 disables)\n",
    "            thinking_budget=-1, \n",
    "            # Include the model's internal thoughts in the response (useful for debugging)\n",
    "            include_thoughts=True, \n",
    "        ),\n",
    "        # Note: candidate_count is currently fixed at 1 for most models/use cases\n",
    "    )\n",
    "\n",
    "  print(\"Layout analysis and transcription process started...\")\n",
    "  prediction, ground_truth = call_gemini(\n",
    "      INSTRUCTION_PROMPT_SIMPLE, \n",
    "      GROUND_TRUTH, model_id=\"gemini-3-pro-preview\", \n",
    "      file_paths=\"Generated Image December 09, 2025 - 4_41PM.jpeg\",\n",
    "      generation_config=generation_config\n",
    "  )\n",
    "  # print(\"Evaluation started...\")\n",
    "  # evaluate_with_gemini(prediction, ground_truth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
